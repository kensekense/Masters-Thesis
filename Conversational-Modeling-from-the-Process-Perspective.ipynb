{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Functions\n",
    "\n",
    "Functions implemented following the paper: \"Context-Aware Trace Clustering\"\n",
    "- Substitution Score Calculation\n",
    "- Insertion Score Calculation\n",
    "- Distance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEPS for Substitution Score\n",
    "\n",
    "#STEP 1: Define the symbols in the list of traces\n",
    "def define_symbols (traces):\n",
    "    assert type(traces) == list\n",
    "    symbols = []\n",
    "    for item in traces:\n",
    "        symbols.append(set(item))\n",
    "    x = symbols[0]\n",
    "    for i in range(len(symbols)):\n",
    "        x = x.union(symbols[i])\n",
    "    \n",
    "    return list(x)\n",
    "\n",
    "\n",
    "#STEP 2: Define the set of all 3-grams in the logs and their frequencies\n",
    "def three_grams (traces):\n",
    "    assert type(traces) == list\n",
    "    g3 = []\n",
    "    g3_freq = {}\n",
    "    for trace in traces:\n",
    "        for i in range(len(trace)-2):\n",
    "            g3.append(\", \".join(list(trace[i:i+3])))\n",
    "            try:\n",
    "                g3_freq[\", \".join(list(trace[i:i+3]))] += 1\n",
    "            except:\n",
    "                g3_freq[\", \".join(list(trace[i:i+3]))] = 1\n",
    "    return list(set(g3)), g3_freq\n",
    "\n",
    "\n",
    "#STEP 3: Define the context for symbol a\n",
    "def define_context(grams):\n",
    "    \n",
    "    assert type(grams) == list\n",
    "    \n",
    "    context = {}\n",
    "    for gram in grams:\n",
    "        x,a,y = gram.split(\", \")\n",
    "        try:\n",
    "            context[a].append(\"{0}, {1}\".format(x,y))\n",
    "        except:\n",
    "            context[a] = []\n",
    "            context[a].append(\"{0}, {1}\".format(x,y))\n",
    "            \n",
    "    #clear dups\n",
    "    for k in list(context.keys()):\n",
    "        context[k] = list(set(context[k]))\n",
    "    \n",
    "    return context\n",
    "\n",
    "\n",
    "#STEP 4: define pairs of context\n",
    "def context_pairs (context):\n",
    "    \n",
    "    assert type(context) == dict\n",
    "    \n",
    "    context_pairs = {}\n",
    "    for a in list(context.keys()):\n",
    "        for b in list(context.keys()):\n",
    "            if a != b:\n",
    "                context_pairs[\"{0}, {1}\".format(a, b)] = list(set(context[a]).intersection(set(context[b])))\n",
    "    \n",
    "    return context_pairs\n",
    "\n",
    "\n",
    "#STEP 5: define co-occurrence combinations\n",
    "def define_cooccurrence(symbols, context_pairs, gram_freq):\n",
    "    \n",
    "    assert type(context_pairs) == dict\n",
    "    assert type(gram_freq) == dict\n",
    "    assert type(symbols) == list\n",
    "    \n",
    "    co_occur = {}\n",
    "    for k in list(context_pairs.keys()):\n",
    "        for item in context_pairs[k]:\n",
    "            for a in symbols:\n",
    "                for b in symbols:\n",
    "                    x,y = item.split(\", \")[0], item.split(\", \")[1]\n",
    "                    if a == b:\n",
    "                        try:\n",
    "                            n = gram_freq[\"{0}, {1}, {2}\".format(x,a,y)]\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = (n*(n-1))/2\n",
    "                        except:\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = 0.0\n",
    "                        \n",
    "                    elif a != b:\n",
    "                        try:\n",
    "                            n_i = gram_freq[\"{0}, {1}, {2}\".format(x,a,y)]\n",
    "                            n_j = gram_freq[\"{0}, {1}, {2}\".format(x,b,y)]\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = n_i*n_j\n",
    "                        except:\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = 0.0\n",
    "    \n",
    "    return co_occur\n",
    "\n",
    "\n",
    "#STEP 6: Define the count of co-occurrences for symbols a,b for all contexts\n",
    "def co_occur_combos(symbols, con_pairs, co_occurs):\n",
    "    assert type(symbols) == list\n",
    "    assert type(con_pairs) == dict\n",
    "    assert type(co_occurs) == dict\n",
    "    \n",
    "    co_occur_combos = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            total = 0.0\n",
    "            for k in list(con_pairs.keys()):\n",
    "                for item in con_pairs[k]:\n",
    "                    total += co_occurs[\"{0}({1}, {2})\".format(item,a,b)]\n",
    "            co_occur_combos[\"{0}, {1}\".format(a,b)] = total\n",
    "    \n",
    "    return co_occur_combos\n",
    "\n",
    "\n",
    "#STEP 7: Define norm on the count of co-occur combos\n",
    "def define_norm (co_combos):\n",
    "    assert type(co_combos) == dict\n",
    "    norm = 0.0\n",
    "    for k in list(co_combos.keys()):\n",
    "        norm += co_combos[k]\n",
    "    \n",
    "    return norm\n",
    "\n",
    "\n",
    "#STEP 8: Define matrix M over A x A\n",
    "def define_matrix (symbols, co_combos, norm):\n",
    "    assert type(symbols) == list\n",
    "    assert type(co_combos) == dict\n",
    "    assert type(norm) == float\n",
    "    \n",
    "    mat_M = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            mat_M[\"{0}, {1}\".format(a,b)] = co_combos[\"{0}, {1}\".format(a,b)]/norm\n",
    "    \n",
    "    return mat_M\n",
    "\n",
    "\n",
    "#STEP 9: Define the probability of occurrence\n",
    "def prob_occur (symbols, mat_M):\n",
    "    assert type(symbols) == list\n",
    "    assert type(mat_M) == dict\n",
    "    \n",
    "    p = {}\n",
    "    for a in symbols:\n",
    "        total = 0\n",
    "        for b in symbols:\n",
    "            if a != b:\n",
    "                total += mat_M[\"{0}, {1}\".format(a,b)]\n",
    "        total += mat_M[\"{0}, {1}\".format(a,a)]\n",
    "        p[\"{0}\".format(a)] = total\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "#STEP 10: Define the expected values\n",
    "def exp_val (symbols, prob):\n",
    "    assert type(symbols) == list\n",
    "    assert type(prob) == dict\n",
    "    \n",
    "    e_val = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            if a == b:\n",
    "                e_val[\"{0}, {1}\".format(a,b)] = prob[\"{0}\".format(a)]**2\n",
    "            else:\n",
    "                e_val[\"{0}, {1}\".format(a,b)] = 2*prob[\"{0}\".format(a)]*prob[\"{0}\".format(b)]\n",
    "    \n",
    "    return e_val\n",
    "\n",
    "\n",
    "#STEP 11: Define the function for substitution scores\n",
    "def sub_scores (traces):\n",
    "    assert type(traces) == list\n",
    "    \n",
    "    symbols = define_symbols(traces)\n",
    "    three_gs, three_gs_freq = three_grams(traces)\n",
    "    cons = define_context(three_gs)\n",
    "    con_pairs = context_pairs(cons)\n",
    "    co_occurs = define_cooccurrence(symbols, con_pairs, three_gs_freq)\n",
    "    co_combos = co_occur_combos(symbols, con_pairs, co_occurs)\n",
    "    norm = define_norm(co_combos)\n",
    "    matM = define_matrix(symbols, co_combos, norm)\n",
    "    probs = prob_occur(symbols, matM)\n",
    "    e_val = exp_val(symbols, probs)\n",
    "    \n",
    "    sub_costs = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            if a!=b:\n",
    "                try:\n",
    "                    sub_costs[\"{0}, {1}\".format(a,b)] = np.log2(matM[\"{0}, {1}\".format(a,b)]/e_val[\"{0}, {1}\".format(a,b)])\n",
    "                except:\n",
    "                    sub_costs[\"{0}, {1}\".format(a,b)] = -1000\n",
    "    \n",
    "    return sub_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEPS 1-3 are the same for Insertion Score\n",
    "\n",
    "#STEP 4: Define occurence of 3-gram counts\n",
    "def occ_count (symbols, cons, grams, gfreq):\n",
    "    assert type(symbols) == list\n",
    "    assert type(grams) == list\n",
    "    assert type(cons) == dict\n",
    "    \n",
    "    o_counts = {}\n",
    "    for a in list(cons.keys()):\n",
    "        for pair in cons[a]:\n",
    "            x = pair.split(\", \")[0]\n",
    "            y = pair.split(\", \")[1]\n",
    "            o_counts[\"{0}, {1}({2})\".format(x,y,a)] = gfreq[\"{0}, {1}, {2}\".format(x,a,y)]\n",
    "    \n",
    "    return o_counts\n",
    "\n",
    "\n",
    "#STEP 5: define countRgivenL\n",
    "def countRgL (symbols, ocounts):\n",
    "    assert type(symbols) == list\n",
    "    assert type(ocounts) == dict\n",
    "    \n",
    "    rgl_counts = {}\n",
    "    \n",
    "    for a in symbols:\n",
    "        for x in symbols:\n",
    "            #if a !=x:\n",
    "            total = 0\n",
    "            for k in list(ocounts.keys()):\n",
    "                if k.split(\"(\")[0].split(\", \")[0] == x and k.split(\"(\")[1] == \"{0})\".format(a):\n",
    "                    total += ocounts[k]\n",
    "            rgl_counts[\"{0}/{1}\".format(a,x)] = total\n",
    "    \n",
    "    return rgl_counts\n",
    "\n",
    "\n",
    "#STEP 6: define norm(a)\n",
    "def rgl_norm (symbols, rgl_counts):\n",
    "    assert type(symbols) == list\n",
    "    assert type(rgl_counts) == dict\n",
    "    \n",
    "    rgl_norms = {}\n",
    "    \n",
    "    for a in symbols:\n",
    "        total = 0\n",
    "        for x in symbols:\n",
    "            #if a !=x:\n",
    "            total += rgl_counts[\"{0}/{1}\".format(a,x)]\n",
    "        rgl_norms[\"{0}\".format(a)] = total\n",
    "    \n",
    "    return rgl_norms\n",
    "\n",
    "\n",
    "#STEP 7: define the probability of all symbols\n",
    "def rgl_prob (trace):\n",
    "    assert type(trace) == list\n",
    "    \n",
    "    p = {}\n",
    "    for item in trace:\n",
    "        for a in item:\n",
    "            try:\n",
    "                p[\"{0}\".format(a)] += 1\n",
    "            except:\n",
    "                p[\"{0}\".format(a)] = 1\n",
    "    \n",
    "    tot_len = 0\n",
    "    for item in trace:\n",
    "        tot_len += len(item)\n",
    "    \n",
    "    for k in list(p.keys()):\n",
    "        p[k] = p[k]/tot_len\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "#STEP 8: define rglNorm\n",
    "def normed_counts (symbols, rgl, norms):\n",
    "    assert type(symbols) == list\n",
    "    assert type(rgl) == dict\n",
    "    assert type(norms) == dict\n",
    "    \n",
    "    normed_rgls = {}\n",
    "    \n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            normed_rgls[\"{0}/{1}\".format(a,b)] = rgl[\"{0}/{1}\".format(a,b)]/norms[\"{0}\".format(a)]\n",
    "    \n",
    "    return normed_rgls\n",
    "\n",
    "\n",
    "#STEP 9: Define the function for insertion score\n",
    "def insert_scores (traces):\n",
    "    assert type(traces) == list\n",
    "    \n",
    "    symbols = define_symbols(traces)\n",
    "    grams, freq = three_grams(traces)\n",
    "    cons = define_context(grams)\n",
    "    oc = occ_count(symbols, cons, grams, freq)\n",
    "    rgl = countRgL(symbols, oc)\n",
    "    norms = rgl_norm(symbols, rgl)\n",
    "    probs = rgl_prob(traces)\n",
    "    norm_rgls = normed_counts(symbols ,rgl, norms)\n",
    "    \n",
    "    scores = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            scores[\"{0}/{1}\".format(a,b)] = np.log2(norm_rgls[\"{0}/{1}\".format(a,b)]/probs[\"{0}\".format(a)]*probs[\"{0}\".format(b)])\n",
    "    \n",
    "    #replace -inf\n",
    "    for k in list(scores.keys()):\n",
    "        if scores[k] == -np.inf:\n",
    "            scores[k] = -1000\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function definition for calculating similarity\n",
    "def calc_similarity(trace1, trace2, sub_cost, ins_cost, probs):\n",
    "    \n",
    "    assert type(trace1) == type(trace2) == list\n",
    "    \n",
    "    #pad traces\n",
    "    trace1 = [\"_\"] + trace1\n",
    "    trace2 = [\"_\"] + trace2\n",
    "    \n",
    "    #set shorter one as tr1\n",
    "    if len(trace1) > len(trace2):\n",
    "        copy = trace1\n",
    "        trace1 = trace2\n",
    "        trace2 = copy\n",
    "\n",
    "    M = len(trace1)\n",
    "    N = len(trace2)\n",
    "    sim_table = np.zeros((M,N)) #establish table\n",
    "    s_score = sub_cost #get substitution score\n",
    "    ins_score = ins_cost #get insertion score\n",
    "    p = probs #get probabilities\n",
    "    \n",
    "    #fill table, horizontal -> vertical\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            \n",
    "            #original fill horizontal\n",
    "            if i == 0:\n",
    "                if j == 0: #first fill\n",
    "                    sim_table[i][j] = 1000\n",
    "                elif j == 1: #first insert\n",
    "                    sim_table[i][j] = p[\"{0}\".format(trace2[j])]\n",
    "                else: #rest fill, base insert scores\n",
    "                    sim_table[i][j] = ins_score[\"{0}/{1}\".format(trace2[j], trace2[j-1])] + sim_table[i][j-1]\n",
    "            \n",
    "            #original fill vertical\n",
    "            elif j == 0:\n",
    "                if i == 0:#first fill\n",
    "                    sim_table[i][j] = 1000\n",
    "                elif i == 1:\n",
    "                    sim_table[i][j] = p[\"{0}\".format(trace1[i])]\n",
    "                else: #rest fill, base is the opposite of insert scores\n",
    "                    sim_table[i][j] = -1*ins_score[\"{0}/{1}\".format(trace1[i], trace1[i-1])] + sim_table[i-1][j]\n",
    "            \n",
    "            elif trace1[i] == trace2[j]: #no changes\n",
    "                sim_table[i][j] = sim_table[i-1][j-1]\n",
    "            \n",
    "            else: #substitution, insertion or deletion\n",
    "                \n",
    "                #determine the min\n",
    "                op = np.argmax([sim_table[i-1][j], sim_table[i][j-1], sim_table[i-1][j-1]]) #in order, removal, insertion, substitution\n",
    "                if op == 0:\n",
    "                    sim_table[i][j] = -1 + sim_table[i-1][j]#-1*ins_score[\"{0}/{1}\".format(trace2[j],trace1[i])] + sim_table[i-1][j] #removal\n",
    "                elif op == 1:\n",
    "                    sim_table[i][j] = ins_score[\"{0}/{1}\".format(trace2[j],trace1[i])] + sim_table[i][j-1] #insertion\n",
    "                elif op == 2:\n",
    "                    sim_table[i][j] = s_score[\"{0}, {1}\".format(trace1[i],trace2[j])] + sim_table[i-1][j-1] #substitution\n",
    "                \n",
    "    return sim_table[i][j] #final score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTL-N Sub-conversations\n",
    "\n",
    "Functions for LTL-N definition, and discovering patterns with long-term dependencies in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to count the occurrences of violations to the label\n",
    "def count_label (sub, labels):\n",
    "    assert type(sub) == list\n",
    "    assert type(labels) == list\n",
    "    \n",
    "    count = 0\n",
    "    for item in sub:\n",
    "        if item not in labels:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "#until-N definition\n",
    "def until_N (trace, x, y, N):\n",
    "    \n",
    "    assert type(trace) == list\n",
    "    assert type(x) == list\n",
    "    assert type(y) == list\n",
    "    \n",
    "    sol = []\n",
    "    current = N\n",
    "    s = -1\n",
    "    e = -1\n",
    "    for i in range(len(trace)):\n",
    "        \n",
    "        if (trace[i] in x) and e == -1 and s == -1: #finding first instance of x\n",
    "            s = i\n",
    "        \n",
    "        if (s != -1) and (trace[i] not in x) and (trace[i] not in y): #started count and violates Until\n",
    "            \n",
    "            if current <= 0: #no more N to give\n",
    "                s = -1\n",
    "                e = -1\n",
    "                current = N\n",
    "                continue #search for next\n",
    "                \n",
    "            else: #more N to give, decrement\n",
    "                current -= 1\n",
    "        \n",
    "        if s != -1 and (trace[i] in y): #found instance of y and x\n",
    "            e = i\n",
    "            sol.append((s,e, count_label(trace[s:e],x), e-s)) #append starting and ending index, with number of appearances of x\n",
    "            s = -1\n",
    "            e = -1\n",
    "    \n",
    "    return sol\n",
    "\n",
    "#weighting function\n",
    "def calc_weights(trace, sub_convos):\n",
    "    \n",
    "    assert type(trace) == list #takes a list of traces, i.e: event log\n",
    "    \n",
    "    tw_id = {}\n",
    "    center = len(trace)//2\n",
    "    tf = {}\n",
    "    idf = {}\n",
    "    N = 0 #track number of sub_convos\n",
    "\n",
    "    #instantiate tf and idf values\n",
    "    for i in range(len(trace)):\n",
    "        tf[i] = 0\n",
    "        idf[i] = 0\n",
    "\n",
    "    #find non-zero from sub_convos table to fill tf and idf table\n",
    "    for key in list(sub_convos.keys()):\n",
    "        if sub_convos[key][0] != 0:\n",
    "            N += 1\n",
    "            for entry in sub_convos[key][1]: #for each entry\n",
    "                s = entry[0]\n",
    "                e = entry[1]\n",
    "                label1, label2 = key.split(\" -> \")\n",
    "\n",
    "                for i in range(s,e):\n",
    "                    if trace[i] == label1 or trace[i] == label2:\n",
    "                        tf[i] += 1\n",
    "                    else:\n",
    "                        idf[i] += 1\n",
    "\n",
    "    #apply log onto idf vals\n",
    "    for i in range(len(idf)):\n",
    "        try:\n",
    "            if idf[i] == 0:\n",
    "                idf[i] = 0\n",
    "            else:\n",
    "                idf[i] = np.log2(N/idf[i])\n",
    "        except:\n",
    "            idf[i] = 0\n",
    "\n",
    "    #calculate weights\n",
    "    for i in range(len(trace)):\n",
    "        if center == i:\n",
    "            tw_id[i] = 1 + (tf[i]*idf[i]) #center\n",
    "        else:\n",
    "            tw_id[i] = (1/np.abs(center-i)) + (tf[i]*idf[i]) #w_i + tf-idf(a_i)\n",
    "\n",
    "    #convert to list\n",
    "    trace_weights = []\n",
    "    for k in list(tw_id.keys()):\n",
    "        trace_weights.append(tw_id[k])\n",
    "        \n",
    "    return trace_weights\n",
    "\n",
    "#weighting function\n",
    "def calc_weights(trace, sub_convos):\n",
    "    \n",
    "    assert type(trace) == list #takes a list of traces, i.e: event log\n",
    "    \n",
    "    tw_id = {}\n",
    "    center = len(trace)//2\n",
    "    tf = {}\n",
    "    idf = {}\n",
    "    N = 0 #track number of sub_convos\n",
    "\n",
    "    #instantiate tf and idf values\n",
    "    for i in range(len(trace)):\n",
    "        tf[i] = 0\n",
    "        idf[i] = 0\n",
    "\n",
    "    #find non-zero from sub_convos table to fill tf and idf table\n",
    "    for key in list(sub_convos.keys()):\n",
    "        if sub_convos[key][0] != 0:\n",
    "            N += 1\n",
    "            for entry in sub_convos[key][1]: #for each entry\n",
    "                s = entry[0]\n",
    "                e = entry[1]\n",
    "                label1, label2 = key.split(\" -> \")\n",
    "\n",
    "                for i in range(s,e):\n",
    "                    if trace[i] == label1 or trace[i] == label2:\n",
    "                        tf[i] += 1\n",
    "                    else:\n",
    "                        idf[i] += 1\n",
    "\n",
    "    #apply log onto idf vals\n",
    "    for i in range(len(idf)):\n",
    "        try:\n",
    "            if idf[i] == 0:\n",
    "                idf[i] = 0\n",
    "            else:\n",
    "                idf[i] = np.log2(N/idf[i])\n",
    "        except:\n",
    "            idf[i] = 0\n",
    "\n",
    "    #calculate weights\n",
    "    for i in range(len(trace)):\n",
    "        if center == i:\n",
    "            tw_id[i] = 1 + (tf[i]*idf[i]) #center\n",
    "        else:\n",
    "            tw_id[i] = (1/np.abs(center-i)) + (tf[i]*idf[i]) #w_i + tf-idf(a_i)\n",
    "\n",
    "    #convert to list\n",
    "    trace_weights = []\n",
    "    for k in list(tw_id.keys()):\n",
    "        trace_weights.append(tw_id[k])\n",
    "        \n",
    "    return trace_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern-Matching\n",
    "\n",
    "Functions for specific pattern matching, using the algorithm PrefixSpan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PrefixSpan variant\n",
    "def create_init_prefix(trace, min_supp):\n",
    "    \n",
    "    assert type(trace) == list\n",
    "    assert type(min_supp) == int\n",
    "    \n",
    "    labels = list(set(trace)) #create the list of symbols\n",
    "    \n",
    "    #create initial prefix\n",
    "    \n",
    "    #chunk trace for sequences\n",
    "    sequences = []\n",
    "    for i in range(0, len(trace), 30):\n",
    "        sequences.append((i,i+30)) #REFERENCE: sequences[i] = (start, end)\n",
    "    \n",
    "    #create bucket for symbols\n",
    "    bucket = {}\n",
    "    for label in labels:\n",
    "        bucket[\"{0}\".format(label)] = 0\n",
    "    \n",
    "    #check for support\n",
    "    for prefix in list(bucket.keys()):\n",
    "        \n",
    "        #check all sequences\n",
    "        for chunk in sequences:\n",
    "            if prefix in trace[chunk[0]:chunk[1]]: #in the sequence\n",
    "                bucket[\"{0}\".format(prefix)] += 1\n",
    "    \n",
    "    #drop lower than min_supp\n",
    "    for key in list(bucket.keys()):\n",
    "        if bucket[key] < min_supp:\n",
    "            bucket.pop(key)\n",
    "    \n",
    "    #create initial projections\n",
    "    projections = {}\n",
    "    \n",
    "    #initialize the list\n",
    "    for key in list(bucket.keys()):\n",
    "        projections[key] = []\n",
    "    \n",
    "    #populate by searching each sequence\n",
    "    for item in list(projections.keys()):\n",
    "        for seq_id in sequences: #should be in the form (start, end)\n",
    "            #look to replace the start index\n",
    "            sequence = trace[seq_id[0]:seq_id[1]]\n",
    "            for i in range(0, len(sequence), len(item)):\n",
    "                if item == sequence[i]: \n",
    "                    projections[item].append((seq_id[0]+i, seq_id[1]))\n",
    "                    break\n",
    "    \n",
    "    #drop entry if projections <= 1\n",
    "    for key in list(projections.keys()):\n",
    "        if len(projections[key]) <= 1:\n",
    "            projections.pop(key)\n",
    "    \n",
    "    return bucket, projections\n",
    "\n",
    "def generate_projections(bucket, trace):\n",
    "    \n",
    "    assert type(bucket) == dict #should be in the form {'use.social.convention': 23}\n",
    "    assert type(trace) == list #should be in the form ['misc', ..., 'use.social.convention']\n",
    "    \n",
    "    projections = {}\n",
    "    sequences = []\n",
    "    for i in range(0, len(trace), 30):\n",
    "        sequences.append((i,i+30)) #REFERENCE: sequences[i] = (start, end)\n",
    "    \n",
    "    #generate a projection for each bucket item\n",
    "    \n",
    "    #initialize the list\n",
    "    for key in list(bucket.keys()):\n",
    "        projections[key] = []\n",
    "    \n",
    "    #populate by searching each sequence\n",
    "    for item in list(projections.keys()):\n",
    "        pref = item.split(\", \")\n",
    "        for seq_id in sequences: #should be in the form (start, end)\n",
    "            #look to replace the start index\n",
    "            sequence = trace[seq_id[0]:seq_id[1]]\n",
    "            for i in range(0, len(sequence), len(pref)):\n",
    "                if pref == sequence[i:i+len(pref)]: \n",
    "                    projections[\"{0}\".format((\", \").join(pref))].append((seq_id[0]+i+len(pref)-1, seq_id[1]))\n",
    "                    break\n",
    "    \n",
    "    #drop if not enough\n",
    "    for k in list(projections.keys()):\n",
    "        if len(projections[k]) <= 1:\n",
    "            projections.pop(k)\n",
    "    \n",
    "    return projections\n",
    "\n",
    "def compute_and_drop(projections, trace, min_supp):\n",
    "\n",
    "    assert type(projections) == dict #should be in the form (\"alpha\": [(start, end), ... (startN, endN)])\n",
    "    assert type(trace) == list\n",
    "    \n",
    "    labels = list(set(trace))\n",
    "    proj_list = []\n",
    "    \n",
    "    #for every projection, count the i+1 patterns\n",
    "    for proj in list(projections.keys()):\n",
    "        #append from the labels to initialize\n",
    "        iplus1_count = {}\n",
    "        for L in labels:\n",
    "            prefix = proj.split(\", \") + [L]\n",
    "            iplus1_count[\"{0}\".format((\", \").join(prefix))] = 0\n",
    "        \n",
    "            #count the labels\n",
    "            sequences = projections[proj]\n",
    "            for seq_id in sequences: #should be in the form (start, end)\n",
    "                sequence = trace[seq_id[0]:seq_id[1]]\n",
    "                for i in range(0, len(sequence), len(prefix)):\n",
    "                    if prefix == sequence[i:i+len(prefix)]:\n",
    "                        iplus1_count[\"{0}\".format((\", \").join(prefix))] += 1\n",
    "                        break\n",
    "\n",
    "            #drop\n",
    "            for k in list(iplus1_count.keys()):\n",
    "                if iplus1_count[k] < min_supp:\n",
    "                    iplus1_count.pop(k)\n",
    "                    \n",
    "        #output to proj_list\n",
    "        proj_list.append(iplus1_count)\n",
    "    \n",
    "    return proj_list       \n",
    "\n",
    "#function to loop generate_proj with compute_and_drop\n",
    "def mine_pattern(trace, min_supp):\n",
    "    \n",
    "    assert type(trace) == list\n",
    "    assert type(min_supp) == int\n",
    "    \n",
    "    patts = {}\n",
    "    docket = []\n",
    "    doc_ind = 0\n",
    "    \n",
    "    #create initial bucket and projections\n",
    "    init_buq, init_proj = create_init_prefix(trace, min_supp)\n",
    "    \n",
    "    docket.append((init_buq, init_proj))\n",
    "    \n",
    "    #loop until empty\n",
    "    while(len(docket) > 0):\n",
    "        \n",
    "        curr_buq= docket[0][0] #current bucket\n",
    "        curr_proj = docket[0][1] #current projections dict for entire bucket\n",
    "\n",
    "        #compute and drop\n",
    "        buckets = compute_and_drop(curr_proj, trace, min_supp)\n",
    "\n",
    "        #for each bucket in buckets\n",
    "        for bucket in buckets:\n",
    "            #compute projection for bucket\n",
    "            next_proj = generate_projections(bucket, trace)\n",
    "            docket.append((bucket, next_proj))\n",
    "            \n",
    "            #add as patt\n",
    "            for item in bucket:\n",
    "                try:\n",
    "                    patts[item] += bucket[item]\n",
    "                except:\n",
    "                    patts[item] = bucket[item]\n",
    "        \n",
    "        docket.pop(0) #finish item and move on\n",
    "    \n",
    "    return patts\n",
    "\n",
    "def get_ranked_patterns (eventlog, top):\n",
    "    \n",
    "    assert type(eventlog) == list\n",
    "    assert type(top) == int\n",
    "    \n",
    "    \n",
    "    #get ITF counts for all traces\n",
    "    itf = {}\n",
    "    for trace in eventlog:\n",
    "        pattern_count = mine_pattern(trace, 2)\n",
    "        for k in list(pattern_count.keys()):\n",
    "            try:\n",
    "                itf[k] += pattern_count[k]\n",
    "            except:\n",
    "                itf[k] = pattern_count[k]\n",
    "\n",
    "    #apply log(N/itf-count)\n",
    "    for k in list(itf.keys()):\n",
    "        itf[k] = np.log(len(itf)/itf[k])\n",
    "\n",
    "    #calc total PF ITF for each trace patterns\n",
    "    pf_itf = {}\n",
    "    for trace in eventlog:\n",
    "        tf = mine_pattern(trace, 2)\n",
    "        for k in list(tf.keys()):\n",
    "            pf_itf[k] = tf[k]*itf[k]\n",
    "\n",
    "    pf_itf = {k: v for k, v in sorted(pf_itf.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    ranked_patterns = []\n",
    "    i = 0\n",
    "    for k in list(pf_itf.keys()):\n",
    "        ranked_patterns.append((k, pf_itf[k]))\n",
    "        i += 1\n",
    "\n",
    "        if i == top:\n",
    "            break\n",
    "    \n",
    "    return ranked_patterns\n",
    "\n",
    "#modify weights\n",
    "def augment_weights(trace, trace_weights, rankings):\n",
    "    \n",
    "    assert type(trace_weights) == list #should be in the form [weight1, weight2, ..., weightN]\n",
    "    assert type(rankings) == list #should be in the form [(pattern1, ranking_weight1), ... (patternN, ranking_weightN)]\n",
    "    \n",
    "    weights = trace_weights.copy()\n",
    "    normalize_val = np.sum(weights)\n",
    "    \n",
    "    markers = {}\n",
    "    for item in rankings:\n",
    "        markers[item[0]] = []\n",
    "    \n",
    "    #search in trace for the top 10 rankings\n",
    "    for item in rankings:\n",
    "        pattern = item[0].split(\", \")\n",
    "        for i in range(0, len(trace), len(pattern)):\n",
    "            if pattern == trace[i:i+len(pattern)]:\n",
    "                markers[item[0]].append((item[1],i, i+len(pattern)-1)) #append (start, end)\n",
    "    \n",
    "    #drop empty\n",
    "    for k in list(markers.keys()):\n",
    "        if len(markers[k]) < 1:\n",
    "            markers.pop(k)\n",
    "    \n",
    "    #use markers to augment weights\n",
    "    for k in list(markers.keys()): #search all patterns\n",
    "        for segment in markers[k]: #for each time pattern occurs\n",
    "            rank_weight = segment[0]/normalize_val\n",
    "            start = segment[1]\n",
    "            end = segment[2]\n",
    "            #augment\n",
    "            for q in range(start, end+1):\n",
    "                weights[q] += rank_weight\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv('./data/graham.norton.s22.e08_data.csv')\n",
    "dat2 = pd.read_csv('./data/graham.norton.s22.e12_data.csv')\n",
    "dat3 = pd.read_csv('./data/blackpink_data.csv')\n",
    "dat4 = pd.read_csv('./data/graham.norton.s22e01.csv')\n",
    "dat5 = pd.read_csv('./data/graham.norton.s22e02.csv')\n",
    "dat6 = pd.read_csv('./data/graham.norton.s22e07.csv')\n",
    "dat7 = pd.read_csv('./data/graham.norton.s22e15.csv')\n",
    "dat8 = pd.read_csv('./data/graham.norton.s22e19.csv')\n",
    "dat9 = pd.read_csv('./data/graham.norton.s24e10.csv')\n",
    "dat10 = pd.read_csv('./data/american_factory.csv')\n",
    "dat11 = pd.read_csv('./data/taylor_swift_miss_americana.csv')\n",
    "dat12 = pd.read_csv('./data/spider-man_into_the_spider-verse.csv')\n",
    "\n",
    "test1 = list(dat1.L) #graham norton\n",
    "test2 = list(dat2.L)\n",
    "test3 = list(dat3.L) #blackpink\n",
    "test4 = list(dat4.L) #graham norton\n",
    "test5 = list(dat5.L)\n",
    "test6 = list(dat6.L)\n",
    "test7 = list(dat7.L)\n",
    "test8 = list(dat8.L)\n",
    "test9 = list(dat9.L)\n",
    "test10 = list(dat10.L) #american factory\n",
    "test11 = list(dat11.L) #taylor swift\n",
    "test12 = list(dat12.L) #spider-verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
