
@article{oroujlou_importance_2012,
	title = {The Importance of Media in Foreign Language Learning},
	volume = {51},
	issn = {1877-0428},
	url = {http://www.sciencedirect.com/science/article/pii/S187704281203251X},
	doi = {https://doi.org/10.1016/j.sbspro.2012.08.113},
	abstract = {The use of mass media to teach language in authentic context represents a double challenge for language teachers. Although media give learners access to authentic language utilized in real life, they convey pre-planned ideology with themselves. The paper clarified the importance of media's authentic language in language learning and teaching and also raised the consciousness of the teachers and learners toward the ideology-laden structures. The researcher used Critical Linguistics and descriptive-analytical method to prove the ideological representation of news structures, and the analysis was done in terms of lexico-grammatical features in order to clarify the role of authentic language in language learning. Author provided some practical and efficient findings that can be applied in language classes to enhance the language and socio-cultural proficiency of the students. It is recommended that teachers provide analytical framework to help students reflect on their language experiences and practices.},
	pages = {24 -- 28},
	journaltitle = {Procedia - Social and Behavioral Sciences},
	author = {Oroujlou, Nasser},
	date = {2012},
	keywords = {and teaching, Language, Learning, Media},
}

@article{r_p_jagadeesh_chandra_bose_context_2009,
	title = {Context Aware Trace Clustering: Towards Improving Process Mining Results},
	doi = {10.1137/1.9781611972795.35},
	abstract = {Process Mining refers to the extraction of process models
from event logs. Real-life processes tend to be less struc-
tured and more °exible. Traditional process mining algo-
rithms have problems dealing with such unstructured pro-
cesses and generate spaghetti-like process models that are
hard to comprehend. An approach to overcome this is to
cluster process instances (a process instance is manifested
as a trace and an event log corresponds to a multi-set of
traces) such that each of the resulting clusters correspond to
a coherent set of process instances that can be adequately
represented by a process model. In this paper, we propose a
context aware approach to trace clustering based on generic
edit distance. It is well known that the generic edit dis-
tance framework is highly sensitive to the costs of edit op-
erations. We de¯ne an automated approach to derive the
costs of edit operations. The method proposed in this paper
outperforms contemporary approaches to trace clustering in
process mining. We evaluate the goodness of the formed
clusters using established ¯tness and comprehensibility met-
rics de¯ned in the context of process mining. The proposed
approach is able to generate clusters such that the process
models mined from the clustered traces show a high degree
of ¯tness and comprehensibility when compared to contem-
porary approaches.},
	journaltitle = {{SIAM} International Conference on Data Mining},
	author = {{R. P. Jagadeesh Chandra Bose} and {Wil M.P. van der Aalst}},
	date = {2009-04},
}

@article{jian_pei_prefixspan_2001,
	title = {{PrefixSpan}: Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth},
	doi = {10.1109/ICDE.2001.914830},
	journaltitle = {Proceedings 17th International Conference on Data Engineering},
	author = {{Jian Pei} and {Jiawei Han} and {Behzad Mortazavi-Asl} and {Helen Pinto} and {Qiming Chen} and {Umeshwar Dayal} and {Mei-Chun Hsu}},
	date = {2001-04-02},
}

@article{denis_kuperberg_linear_2017,
	title = {{LINEAR} {TEMPORAL} {LOGIC} {FOR} {REGULAR} {COST} {FUNCTIONS}},
	doi = {10.2168/LMCS-???},
	abstract = {Regular cost functions have been introduced recently as an extension to the
notion of regular languages with counting capabilities, which retains strong closure, equivalence,
and decidability properties. The specificity of cost functions is that exact values
are not considered, but only estimated.
In this paper, we define an extension of Linear Temporal Logic ({LTL}) over finite words
to describe cost functions. We give an explicit translation from this new logic to two
dual form of cost automata, and we show that the natural decision problems for this logic
are {PSPACE}-complete, as it is the case in the classical setting. We then algebraically
characterize the expressive power of this logic, using a new syntactic congruence for cost
functions introduced in this paper.},
	journaltitle = {{LOGICAL} {METHODS} {IN} {COMPUTER} {SCIENCE}},
	author = {{DENIS KUPERBERG}},
	date = {2017-02-08},
}

@article{marsha_chechik_events_1999,
	title = {Events in Property Patterns},
	doi = {arXiv:cs/9906029v2},
	abstract = {pattern-based approach to the presentation, codification
and reuse of property specifications for finite-state verification was pro-
posed by Dwyer and his colleagues in [4,3]. The patterns enable non-
experts to read and write formal specifications for realistic systems and
facilitate easy conversion of specifications between formalisms, such as
{LTL}, {CTL}, {QRE}. In this paper we extend the pattern system with events
— changes of values of variables in the context of {LTL}.},
	author = {{Marsha Chechik} and {Dimitrie O. P˘aun}},
	date = {1999-06-29},
}

@article{tomas_mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	doi = {arXiv:1301.3781v3},
	abstract = {We propose two novel model architectures for computing continuous vector representations
of words from very large data sets. The quality of these representations
is measured in a word similarity task, and the results are compared to the previously
best performing techniques based on different types of neural networks. We
observe large improvements in accuracy at much lower computational cost, i.e. it
takes less than a day to learn high quality word vectors from a 1.6 billion words
data set. Furthermore, we show that these vectors provide state-of-the-art performance
on our test set for measuring syntactic and semantic word similarities.},
	author = {{Tomas Mikolov} and {Kai Chen} and {Greg Corrado} and {Jeffrey Dean}},
	date = {2013-09-07},
}

@article{ilya_sutskever_sequence_2014,
	title = {Sequence to Sequence Learning with Neural Networks},
	doi = {arXiv:1409.3215v3},
	abstract = {Deep Neural Networks ({DNNs}) are powerful models that have achieved excellent
performance on difficult learning tasks. Although {DNNs} work well whenever
large labeled training sets are available, they cannot be used to map sequences to
sequences. In this paper, we present a general end-to-end approach to sequence
learning that makes minimal assumptions on the sequence structure. Our method
uses a multilayered Long Short-{TermMemory} ({LSTM}) to map the input sequence
to a vector of a fixed dimensionality, and then another deep {LSTM} to decode the
target sequence from the vector. Our main result is that on an English to French
translation task {fromtheWMT}’14 dataset, the translations produced by the {LSTM}
achieve a {BLEU} score of 34.8 on the entire test set, where the {LSTM}’s {BLEU}
score was penalized on out-of-vocabulary words. Additionally, the {LSTM} did not
have difficulty on long sentences. For comparison, a phrase-based {SMT} system
achieves a {BLEU} score of 33.3 on the same dataset. When we used the {LSTM}
to rerank the 1000 hypotheses produced by the aforementioned {SMT} system, its
{BLEU} score increases to 36.5, which is close to the previous best result on this
task. The {LSTM} also learned sensible phrase and sentence representations that
are sensitive to word order and are relatively invariant to the active and the passive
voice. Finally, we found that reversing the order of the words in all source
sentences (but not target sentences) improved the {LSTM}’s performancemarkedly,
because doing so introduced many short term dependencies between the source
and the target sentence which made the optimization problem easier.},
	author = {{Ilya Sutskever} and {Oriol Vinyals} and {Quoc V. Le}},
	date = {2014-12-14},
}

@article{ian_j_goodfellow_generative_2014,
	title = {Generative Adversarial Nets},
	doi = {arXiv:1406.2661v1},
	abstract = {We propose a new framework for estimating generative models via an adversarial
process, in which we simultaneously train two models: a generative model G
that captures the data distribution, and a discriminative model D that estimates
the probability that a sample came from the training data rather than G. The training
procedure for G is to maximize the probability of D making a mistake. This
framework corresponds to a minimax two-player game. In the space of arbitrary
functions G and D, a unique solution exists, with G recovering the training data
distribution and D equal to 1
2 everywhere. In the case where G and D are defined
by multilayer perceptrons, the entire system can be trained with backpropagation.
There is no need for any Markov chains or unrolled approximate inference networks
during either training or generation of samples. Experiments demonstrate
the potential of the framework through qualitative and quantitative evaluation of
the generated samples.},
	author = {{Ian J. Goodfellow} and {Jean Pouget-Abadie} and {Mehdi Mirza} and {Bing Xu} and {David Warde-Farley} and {Sherjil Ozair} and {Aaron Courville} and {Yoshua Bengio}},
	date = {2014-06-10},
}

@article{roman_vasiliev_tracesim_2020,
	title = {{TraceSim}: A Method for Calculating Stack Trace Similarity},
	doi = {arXiv:2009.12590v1},
	abstract = {Many contemporary software products have subsystems
for automatic crash reporting. However, it is well-known
that the same bug can produce slightly different reports. To
manage this problem, reports are usually grouped, often manually
by developers. Manual triaging, however, becomes infeasible
for products that have large userbases, which is the reason for
many different approaches to automating this task. Moreover, it
is important to improve quality of triaging due to the big volume
of reports that needs to be processed properly. Therefore, even
a relatively small improvement could play a significant role in
overall accuracy of report bucketing. The majority of existing
studies use some kind of a stack trace similarity metric, either
based on information retrieval techniques or string matching
methods. However, it should be stressed that the quality of
triaging is still insufficient.
In this paper, we describe {TraceSim} — a novel approach
to address this problem which combines {TF}-{IDF}, Levenshtein
distance, and machine learning to construct a similarity metric.
Our metric has been implemented inside an industrial-grade
report triaging system. The evaluation on a manually labeled
dataset shows significantly better results compared to baseline
approaches.},
	author = {{Roman Vasiliev} and {Dmitrij Koznov} and {George Chernishev} and {Aleksandr Khvorov} and {Dmitry Luciv} and {Nikita Povarov}},
	date = {2020-09-26},
}

@article{tom_young_recent_2018,
	title = {Recent Trends in Deep Learning Based Natural Language Processing},
	doi = {arXiv:1708.02709v8},
	abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced
state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of
natural language processing ({NLP}). In this paper, we review significant deep learning related models and methods that have been
employed for numerous {NLP} tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the
various models and put forward a detailed understanding of the past, present and future of deep learning in {NLP}.},
	author = {{Tom Young} and {Devamanyu Hazarika} and {Soujanya Poria} and {Erik Cambria}},
	date = {2018-11-25},
}

@article{prakash_m_nadkarni_natural_2011,
	title = {Natural language processing: an introduction},
	doi = {10.1136/amiajnl-2011-000464},
	abstract = {Objectives To provide an overview and tutorial of natural
language processing ({NLP}) and modern {NLP}-system
design.
Target audience This tutorial targets the medical
informatics generalist who has limited acquaintance with
the principles behind {NLP} and/or limited knowledge of
the current state of the art.
Scope We describe the historical evolution of {NLP}, and
summarize common {NLP} sub-problems in this extensive
field. We then provide a synopsis of selected highlights
of medical {NLP} efforts. After providing a brief description
of common machine-learning approaches that are being
used for diverse {NLP} sub-problems, we discuss how
modern {NLP} architectures are designed, with a summary
of the Apache Foundation’s Unstructured Information
Management Architecture. We finally consider possible
future directions for {NLP}, and reflect on the possible
impact of {IBM} Watson on the medical field.},
	journaltitle = {J Am Med Inform Assoc},
	author = {{Prakash M Nadkarni} and {Lucila Ohno-Machado} and {Wendy W Chapman}},
	date = {2011},
}

@article{nijat_mehdiyev_explainable_2020,
	title = {Explainable Artificial Intelligence for Process Mining: A General Overview and Application of a Novel Local Explanation Approach for Predictive Process Monitoring},
	abstract = {The contemporary process-aware information systems possess the capabilities
to record the activities generated during the process execution. To leverage
these process specific fine-granular data, process mining has recently
emerged as a promising research discipline. As an important branch of process
mining, predictive business process management, pursues the objective to generate
forward-looking, predictive insights to shape business processes. In this
study, we propose a conceptual framework sought to establish and promote understanding
of decision-making environment, underlying business processes and
nature of the user characteristics for developing explainable business process prediction
solutions. Consequently, with regard to the theoretical and practical implications
of the framework, this study proposes a novel local post-hoc explanation
approach for a deep learning classifier that is expected to facilitate the domain
experts in justifying the model decisions. In contrary to alternative popular
perturbation-based local explanation approaches, this study defines the local regions
from the validation dataset by using the intermediate latent space representations
learned by the deep neural networks. To validate the applicability of the
proposed explanation method, the real-life process log data delivered by the
Volvo {IT} Belgium’s incident management system are used. The adopted deep
learning classifier achieves a good performance with the Area Under the {ROC}
Curve of 0.94. The generated local explanations are also visualized and presented
with relevant evaluation measures that are expected to increase the users’ trust in
the black-box-model.},
	journaltitle = {Springer},
	author = {{Nijat Mehdiyev} and {Peter Fettke}},
	date = {2020-10-07},
}

@article{marie_escribe_human_2019,
	title = {Human Evaluation of Neural Machine Translation: The Case of Deep Learning},
	doi = {10.26615/issn.2683-0078.2019_005},
	abstract = {Recent advances in artificial neural
networks now have a great impact on
translation technology. A considerable
achievement was reached in this field with
the publication of L’Apprentissage Profond.
This book, originally written in English
(Deep Learning), was entirely machinetranslated
into French and post-edited by
several experts. In this context, it appears
essential to have a clear vision of the
performance of {MT} tools. Providing an
evaluation of {NMT} is precisely the aim of
the present research paper. To accomplish
this objective, a framework for error
categorisation was built and a comparative
analysis of the raw translation output and
the post-edited version was performed with
the purpose of identifying recurring
patterns of errors. The findings showed that
even though some grammatical errors were
spotted, the output was generally correct
from a linguistic point of view. The most
recurring errors are linked to the
specialised terminology employed in this
book. Further errors include parts of text
that were not translated as well as edits
based on stylistic preferences. The major
part of the output was not acceptable as
such and required several edits per segment,
but some sentences were of publishable
quality and were therefore left untouched
in the final version.},
	journaltitle = {Proceedings of the 2nd Workshop on Human-Informed Translation and Interpreting Technology (},
	author = {{Marie Escribe}},
	date = {2019-09-05},
}

@article{anton_osika_second_2018,
	title = {Second Language Acquisition Modeling: An Ensemble Approach},
	doi = {arXiv:1806.04525v1},
	abstract = {Accurate prediction of students knowledge is
a fundamental building block of personalized
learning systems. Here, we propose a novel
ensemble model to predict student knowledge
gaps. Applying our approach to student
trace data from the online educational platform
Duolingo we achieved highest score on both
evaluation metrics for all three datasets in the
2018 Shared Task on Second Language Acquisition
Modeling. We describe our model and
discuss relevance of the task compared to how
it would be setup in a production environment
for personalized education.},
	author = {{Anton Osika} and {Susanna Nilsson} and {Andrii Sydorchuk} and {Faruk Sahin} and {Anders Huss}},
	date = {2018-06-09},
}

@article{majid_rafiei_towards_2020,
	title = {Towards Quantifying Privacy in Process Mining},
	doi = {arXiv:2012.12031v1},
	abstract = {Process mining employs event logs to provide insights into
the actual processes. Event logs are recorded by information systems and
contain valuable information helping organizations to improve their processes.
However, these data also include highly sensitive private information
which is a major concern when applying process mining. Therefore,
privacy preservation in process mining is growing in importance, and
new techniques are being introduced. The eectiveness of the proposed
privacy preservation techniques needs to be evaluated. It is important to
measure both sensitive data protection and data utility preservation. In
this paper, we propose an approach to quantify the eectiveness of privacy
preservation techniques. We introduce two measures for quantifying
disclosure risks to evaluate the sensitive data protection aspect. Moreover,
a measure is proposed to quantify data utility preservation for the
main process mining activities. The proposed measures have been tested
using various real-life event logs.},
	author = {{Majid Rafiei} and {Wil M.P. van der}},
	date = {2020-12-21},
}

@article{majid_rafiei_privacy-preserving_2021,
	title = {Privacy-Preserving Data Publishing in Process Mining},
	doi = {arXiv:2101.02627v1},
	abstract = {Process mining aims to provide insights into the actual pro-
cesses based on event data. These data are often recorded by information
systems and are widely available. However, they often contain sensitive
private information that should be analyzed responsibly. Therefore, pri-
vacy issues in process mining are recently receiving more attention. Pri-
vacy preservation techniques obviously need to modify the original data,
yet, at the same time, they are supposed to preserve the data utility.
Privacy-preserving transformations of the data may lead to incorrect or
misleading analysis results. Hence, new infrastructures need to be de-
signed for publishing the privacy-aware event data whose aim is to pro-
vide metadata regarding the privacy-related transformations on event
data without revealing details of privacy preservation techniques or the
protected information. In this paper, we provide formal denitions for
the main anonymization operations, used by privacy models in process
mining. These are used to create an infrastructure for recording the pri-
vacy metadata. We advocate the proposed privacy metadata in practice
by designing a privacy extension for the {XES} standard and a general data
structure for event data which are not in the form of standard event logs.},
	author = {{Majid Rafiei} and {Wil M.P. van der Aalst}},
	date = {2021-01-04},
}

@article{cynthia_white_distance_2006,
	title = {Distance learning of foreign languages},
	doi = {10.1017/S0261444806003727},
	abstract = {This article provides a critical overview of the field
of distance language learning, challenging the way in
which the field is often narrowly conceptualised as the
development of technology-mediated language learning
opportunities. Early sections focus on issues of concept and
definition and both theoretical and pedagogical perspectives
on the field. Emphasis is placed on evident shifts from a
concern with structural and organisational issues to a focus
on transactional issues associated with teaching/learning
opportunities within emerging paradigms for distance
language learning. The next section reviews choices
and challenges in incorporating technology into distance
language learning environments, foregrounding decisions
about technology made in particular sociocultural contexts,
the contribution of ‘low-end’ technologies and research
directions in developing new learning spaces and in
using online technologies. The investigation of learner
contributions to distance language learning is an important
avenue of enquiry in the field, given the preoccupation with
technology and virtual learning environments, and this is
the subject of section six. The two final sections identify
future research directions and provide a series of conclusions
about research and practice in distance language learning
as technology-mediated interactions increasingly come to
influence the way we think about the processes of language
learning and teaching.},
	journaltitle = {Cambridge University Press},
	author = {{Cynthia White}},
	date = {2006},
}

@article{robert_j_blake_use_2009,
	title = {The Use of Technology for Second Language Distance Learning},
	doi = {0026-7902/09/822–835},
	abstract = {This article describes distance learning ({DL}) for languages within the context of recent advances
and research findings in computer-assisted language learning ({CALL}). In addition to
reviewing the different {DL} modalities, theoretical underpinnings, and the most appropriate
technological applications to second language learning, the issues of conducting {DL} research
and training faculty to work with this new learning environment will be examined. Garrett’s
(1991) overview of an earlier state of the {CALL} field serves as the background for judging how
far the field has come, especially with respect to the pedagogical challenges, which have not
changed drastically since the 1990s. A key consideration is the notion of interactivity, which is
analyzed in depth with relation to both tutorial {CALL} and social computing.},
	journaltitle = {The Modern Language Journal},
	author = {{ROBERT J. BLAKE}},
	date = {2009},
}

@article{emily_chen_student_2020,
	title = {Student perceptions of distance learning strategies during {COVID}-19},
	doi = {10.1002/jdd.12339},
	journaltitle = {American Dental Education Association},
	author = {{Emily Chen} and {Kristie Kaczmarek} and {Hiroe Ohyama}},
	date = {2020-07-01},
}

@article{guillaume_lample_unsupervised_2018,
	title = {{UNSUPERVISED} {MACHINE} {TRANSLATION} {USING} {MONOLINGUAL} {CORPORA} {ONLY}},
	doi = {arXiv:1711.00043v2},
	abstract = {Machine translation has recently achieved impressive performance thanks to recent
advances in deep learning and the availability of large-scale parallel corpora.
There have been numerous attempts to extend these successes to low-resource language
pairs, yet requiring tens of thousands of parallel sentences. In this work, we
take this research direction to the extreme and investigate whether it is possible to
learn to translate even without any parallel data. We propose a model that takes
sentences from monolingual corpora in two different languages and maps them
into the same latent space. By learning to reconstruct in both languages from this
shared feature space, the model effectively learns to translate without using any
labeled data. We demonstrate our model on two widely used datasets and two language
pairs, reporting {BLEU} scores of 32.8 and 15.1 on the Multi30k and {WMT}
English-French datasets, without using even a single parallel sentence at training
time.},
	journaltitle = {{ICLR} 2018},
	author = {{Guillaume Lample} and {Alexis Conneau} and {Ludovic Denoyer} and {Marc’Aurelio Ranzato}},
	date = {2018-04-13},
}

@book{wil_van_der_aalst_process_2016,
	edition = {2},
	title = {Process Mining Data Science in Action},
	isbn = {978-3-662-49850-7},
	pagetotal = {467},
	publisher = {Springer},
	author = {{Wil van der Aalst}},
	date = {2016},
}