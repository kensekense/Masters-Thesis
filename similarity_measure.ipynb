{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit-distance operations are defined as \n",
    "* (a,a) denotes a match of symbols at the given position\n",
    "* (a,-) denotes deletion of symbol 'a' at some position\n",
    "* (-,b) denotes insertion of symbol 'b' at some position\n",
    "* (a,b) denotes replacement of 'a' with 'b' at some position, and a != b\n",
    "We assign a separate cost for each of these operations according to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity and completeness sake, we define a simple levenshtein distance function first\n",
    "\n",
    "def basic_distance (trace1, trace2):\n",
    "\n",
    "    M = len(trace1)\n",
    "    N = len(trace2)\n",
    "    edit_table = np.zeros((M,N)) #establish table\n",
    "\n",
    "    #fill table\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "\n",
    "            if i == 0:\n",
    "                edit_table[i][j] = j\n",
    "\n",
    "            elif j ==0:\n",
    "                edit_table[i][j] = i\n",
    "\n",
    "            elif trace1[i-1] == trace2[j-1]:\n",
    "                edit_table[i][j] = edit_table[i-1][j-1]\n",
    "\n",
    "            else:\n",
    "                edit_table[i][j] = 1 + min(edit_table[i-1][j], edit_table[i][j-1], edit_table[i-1][j-1]) #scoring done here\n",
    "\n",
    "    return edit_table[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample test\n",
    "dat1 = pd.read_csv('./graham.norton.s22.e08_data.csv')\n",
    "dat2 = pd.read_csv('./graham.norton.s22.e12_data.csv')\n",
    "test1 = list(dat1.L)\n",
    "test2 = list(dat2.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_distance(test1,test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline\n",
    "import nltk\n",
    "nltk.edit_distance(test1,test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have to alter the function to accomodate for different scoring metrics according to the paper\n",
    "* substitution of uncorrelated activities should be discouraged\n",
    "* substitution of contrasting activities should be penalized\n",
    "* insertion of activities out of context should be discouraged\n",
    "* substitution of correlated activities should be encouraged in proportion to the degree of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity is calculated to address substitution costs\n",
    "def sub_cost (log):\n",
    "    \n",
    "    g3_freq = {}\n",
    "    conteX = {}\n",
    "    \n",
    "    '''\n",
    "    g3_freq is a dictionary in the form:\n",
    "    key: triple of labels, i.e: \"respond.agree, open.question, give.opinion\"\n",
    "    value: int of frequency value, i.e: 20\n",
    "    \n",
    "    conteX is a dictionary in the form:\n",
    "    key: single or pair label, i.e: \"respond.agree\", \"respond.agree, open.question\"\n",
    "    value: list of label pairs\n",
    "    value: set of label pairs, i.e: [\"respond.agree, give.opinion\", \"closed.question, relax.atmosphere\"]\n",
    "    **value depends on whether it is a singular or pair label used as a key\n",
    "    \n",
    "    co_occur is a dictionary in the form:\n",
    "    key: a context and label pair, i.e: \"open.question, give.opinion\"(\"relax.atmosphere\", \"use.social.convention\")\n",
    "    value: int of the co-occurence value i.e: 20\n",
    "    \n",
    "    '''\n",
    "   \n",
    "    #STEP 1: let A be the alphabet\n",
    "    symbols = set(log)\n",
    "    \n",
    "    #set conteX\n",
    "    for item in symbols:\n",
    "        conteX[item] = []\n",
    "        \n",
    "    #STEP 2: find the 3 grams and their freq \n",
    "    for i in range(len(log)-3):\n",
    "        try:\n",
    "            g3_freq[\", \".join(list(log[i:i+3]))] += 1\n",
    "        except:\n",
    "            g3_freq[\", \".join(list(log[i:i+3]))] = 1\n",
    "    \n",
    "    #STEP 3: determine the set of contexts for each symbols from the 3-grams\n",
    "    for threegram in list(g3_freq.keys()):\n",
    "        con_a, sym, con_b = threegram.split(\", \")\n",
    "        conteX[sym].append(con_a + \", \" + con_b)\n",
    "\n",
    "    #STEP 4: determine context for each pair\n",
    "    for p1 in symbols:\n",
    "        for p2 in symbols:\n",
    "            conteX[\"{0}, {1}\".format(p1, p2)] = list(set(conteX[p1] + conteX[p2]))\n",
    "\n",
    "    #STEP 5: determine co-occurrence\n",
    "    co_occur = {}\n",
    "    for p1 in symbols:\n",
    "        for p2 in symbols:\n",
    "            if p1 != p2:\n",
    "                for con in conteX[\"{0}, {1}\".format(p1,p2)]:\n",
    "                    #consider the 3-grams for each of the context in this pair\n",
    "                    try:\n",
    "                        co_occur[\"{0}({1}, {2})\".format(con,p1,p2)] = g3_freq[\"{0}, {1}, {2}\".format(con.split(\", \")[0], p1, con.split(\", \")[1])] * g3_freq[\"{0}, {1}, {2}\".format(con.split(\", \")[0], p2, con.split(\", \")[1])]\n",
    "                    except:\n",
    "                        co_occur[\"{0}({1}, {2})\".format(con,p1,p2)] = 0\n",
    "            elif p1 == p2:\n",
    "                for con in conteX[\"{0}, {1}\".format(p1,p2)]:\n",
    "                    try:\n",
    "                        co_occur[\"{0}({1}, {2})\".format(con,p1,p2)] = (g3_freq[\"{0}, {1}, {2}\".format(con.split(\", \")[0], p1, con.split(\", \")[1])]*(g3_freq[\"{0}, {1}, {2}\".format(con.split(\", \")[0], p1, con.split(\", \")[1])]-1))//2\n",
    "                    except:\n",
    "                        co_occur[\"{0}({1}, {2})\".format(con,p1,p2)] = 0\n",
    "    #STEP 6: calculate co-occurrence combinations\n",
    "    co_occur_combo = {}\n",
    "    for p1 in symbols:\n",
    "        for p2 in symbols:\n",
    "            total = 0\n",
    "            for con in conteX[\"{0}, {1}\".format(p1,p2)]:\n",
    "                total += co_occur[\"{0}({1}, {2})\".format(con, p1, p2)]\n",
    "            co_occur_combo[\"{0}, {1}\".format(p1, p2)] = total\n",
    "        \n",
    "    #STEP 7: define the norm of all the count of co-occurence combinations\n",
    "    norm = 0\n",
    "    for k in co_occur_combo.keys():\n",
    "        norm += co_occur_combo[k]\n",
    "    \n",
    "    #STEP 8: define the matrix\n",
    "    mat = {}\n",
    "    for p1 in symbols:\n",
    "        for p2 in symbols:\n",
    "            mat[\"{0}, {1}\".format(p1,p2)] = co_occur_combo[\"{0}, {1}\".format(p1, p2)]/norm\n",
    "    \n",
    "    #STEP 9: define probability of occurrence\n",
    "    p_occur = {}\n",
    "    for s in symbols:\n",
    "        par = 1\n",
    "        for b in symbols:\n",
    "            if s != b:\n",
    "                try:\n",
    "                    p_occur[s] += mat[\"{0}, {1}\".format(s,b)]\n",
    "                    par += 1\n",
    "                except:\n",
    "                    p_occur[s] = 0\n",
    "        p_occur[s] += mat[\"{0}, {1}\".format(s,s)]\n",
    "        p_occur[s] = p_occur[s]/par\n",
    "    \n",
    "    #STEP 10: define expected values matrix\n",
    "    E_mat = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            if a == b:\n",
    "                E_mat[\"{0}, {1}\".format(a,b)] = p_occur[a]**2\n",
    "            else:\n",
    "                E_mat[\"{0}, {1}\".format(a,b)] = 2*p_occur[a]*p_occur[b]\n",
    "    \n",
    "    #STEP 11: define matrix of scores\n",
    "    scores = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            if a != b:\n",
    "                scores[\"{0}, {1}\".format(a,b)] = np.log2(mat[\"{0}, {1}\".format(a,b)]/E_mat[\"{0}, {1}\".format(a,b)])\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'use.social.convention, respond.agree': 4.307970174951491, 'use.social.convention, relax.atmosphere': 5.522587734962121, 'use.social.convention, give.opinion': 5.0279074603308755, 'use.social.convention, x': 1.680969281578785, 'use.social.convention, deflection': 3.7710490200474336, 'use.social.convention, closed.question': 4.130652341532316, 'use.social.convention, open.question': 4.467716233974865, 'use.social.convention, respond.deny': 3.820834443562954, 'respond.agree, use.social.convention': 4.307970174951491, 'respond.agree, relax.atmosphere': 5.200470411838418, 'respond.agree, give.opinion': 5.296264655822938, 'respond.agree, x': 0.9953794112482077, 'respond.agree, deflection': 5.995774832667948, 'respond.agree, closed.question': 5.524601236186945, 'respond.agree, open.question': 5.669793858904678, 'respond.agree, respond.deny': 7.2558498173775, 'relax.atmosphere, use.social.convention': 5.522587734962121, 'relax.atmosphere, respond.agree': 5.200470411838418, 'relax.atmosphere, give.opinion': 5.309052457010987, 'relax.atmosphere, x': 1.8098281189661627, 'relax.atmosphere, deflection': 4.405143165685233, 'relax.atmosphere, closed.question': 5.506604040884466, 'relax.atmosphere, open.question': 5.356216103496346, 'relax.atmosphere, respond.deny': 5.316120836004553, 'give.opinion, use.social.convention': 5.0279074603308755, 'give.opinion, respond.agree': 5.296264655822938, 'give.opinion, relax.atmosphere': 5.309052457010987, 'give.opinion, x': 2.495916805406235, 'give.opinion, deflection': 5.386687735651477, 'give.opinion, closed.question': 5.081692307432468, 'give.opinion, open.question': 5.0666498891819645, 'give.opinion, respond.deny': 4.723890545331816, 'x, use.social.convention': 1.680969281578785, 'x, respond.agree': 0.9953794112482077, 'x, relax.atmosphere': 1.8098281189661627, 'x, give.opinion': 2.495916805406235, 'x, deflection': 1.4983175822080188, 'x, closed.question': 0.5267149952175282, 'x, open.question': 0.49096962377053077, 'x, respond.deny': 0.1272598847634896, 'deflection, use.social.convention': 3.7710490200474336, 'deflection, respond.agree': 5.995774832667948, 'deflection, relax.atmosphere': 4.405143165685233, 'deflection, give.opinion': 5.386687735651477, 'deflection, x': 1.4983175822080188, 'deflection, closed.question': 5.069562622042463, 'deflection, open.question': 3.966703054736929, 'deflection, respond.deny': 6.602993315729888, 'closed.question, use.social.convention': 4.130652341532316, 'closed.question, respond.agree': 5.524601236186945, 'closed.question, relax.atmosphere': 5.506604040884466, 'closed.question, give.opinion': 5.081692307432468, 'closed.question, x': 0.5267149952175282, 'closed.question, deflection': 5.069562622042463, 'closed.question, open.question': 7.185865812470611, 'closed.question, respond.deny': 6.04642822801824, 'open.question, use.social.convention': 4.467716233974865, 'open.question, respond.agree': 5.669793858904678, 'open.question, relax.atmosphere': 5.356216103496346, 'open.question, give.opinion': 5.0666498891819645, 'open.question, x': 0.49096962377053077, 'open.question, deflection': 3.966703054736929, 'open.question, closed.question': 7.185865812470611, 'open.question, respond.deny': 5.368234861189327, 'respond.deny, use.social.convention': 3.820834443562954, 'respond.deny, respond.agree': 7.2558498173775, 'respond.deny, relax.atmosphere': 5.316120836004553, 'respond.deny, give.opinion': 4.723890545331816, 'respond.deny, x': 0.1272598847634896, 'respond.deny, deflection': 6.602993315729888, 'respond.deny, closed.question': 6.04642822801824, 'respond.deny, open.question': 5.368234861189327}\n"
     ]
    }
   ],
   "source": [
    "print(sub_cost(test1+test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
