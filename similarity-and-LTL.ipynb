{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivations\n",
    "\n",
    "We're on the stage where we're looking for correlation between trace similarity and exhibiting similar LTL expressions. Here, we combine the code we used in the two previous sections in order to further develop sub-conversation exploration/discovery from notions of similarity based on the modified edit-distance/substitution and insertion scores.\n",
    "\n",
    "# Functions\n",
    "\n",
    "Below we'll store the functions used, written originally and copied over from the other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to count the occurrences of violations to the label\n",
    "def count_label (sub, labels):\n",
    "    assert type(sub) == list\n",
    "    assert type(labels) == list\n",
    "    \n",
    "    count = 0\n",
    "    for item in sub:\n",
    "        if item not in labels:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "#until-N definition\n",
    "def until_N (trace, x, y, N):\n",
    "    \n",
    "    assert type(trace) == list\n",
    "    assert type(x) == list\n",
    "    assert type(y) == list\n",
    "    \n",
    "    sol = []\n",
    "    current = N\n",
    "    s = -1\n",
    "    e = -1\n",
    "    for i in range(len(trace)):\n",
    "        \n",
    "        if (trace[i] in x) and e == -1 and s == -1: #finding first instance of x\n",
    "            s = i\n",
    "        \n",
    "        if (s != -1) and (trace[i] not in x) and (trace[i] not in y): #started count and violates Until\n",
    "            \n",
    "            if current <= 0: #no more N to give\n",
    "                s = -1\n",
    "                e = -1\n",
    "                current = N\n",
    "                continue #search for next\n",
    "                \n",
    "            else: #more N to give, decrement\n",
    "                current -= 1\n",
    "        \n",
    "        if s != -1 and (trace[i] in y): #found instance of y and x\n",
    "            e = i\n",
    "            sol.append((s,e, count_label(trace[s:e],x), e-s)) #append starting and ending index, with number of appearances of x\n",
    "            s = -1\n",
    "            e = -1\n",
    "    \n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEPS for Substitution Score\n",
    "\n",
    "#STEP 1: Define the symbols in the list of traces\n",
    "def define_symbols (traces):\n",
    "    assert type(traces) == list\n",
    "    symbols = []\n",
    "    for item in traces:\n",
    "        symbols.append(set(item))\n",
    "    x = symbols[0]\n",
    "    for i in range(len(symbols)):\n",
    "        x = x.union(symbols[i])\n",
    "    \n",
    "    return list(x)\n",
    "\n",
    "\n",
    "#STEP 2: Define the set of all 3-grams in the logs and their frequencies\n",
    "def three_grams (traces):\n",
    "    assert type(traces) == list\n",
    "    g3 = []\n",
    "    g3_freq = {}\n",
    "    for trace in traces:\n",
    "        for i in range(len(trace)-2):\n",
    "            g3.append(\", \".join(list(trace[i:i+3])))\n",
    "            try:\n",
    "                g3_freq[\", \".join(list(trace[i:i+3]))] += 1\n",
    "            except:\n",
    "                g3_freq[\", \".join(list(trace[i:i+3]))] = 1\n",
    "    return list(set(g3)), g3_freq\n",
    "\n",
    "\n",
    "#STEP 3: Define the context for symbol a\n",
    "def define_context(grams):\n",
    "    \n",
    "    assert type(grams) == list\n",
    "    \n",
    "    context = {}\n",
    "    for gram in grams:\n",
    "        x,a,y = gram.split(\", \")\n",
    "        try:\n",
    "            context[a].append(\"{0}, {1}\".format(x,y))\n",
    "        except:\n",
    "            context[a] = []\n",
    "            context[a].append(\"{0}, {1}\".format(x,y))\n",
    "            \n",
    "    #clear dups\n",
    "    for k in list(context.keys()):\n",
    "        context[k] = list(set(context[k]))\n",
    "    \n",
    "    return context\n",
    "\n",
    "\n",
    "#STEP 4: define pairs of context\n",
    "def context_pairs (context):\n",
    "    \n",
    "    assert type(context) == dict\n",
    "    \n",
    "    context_pairs = {}\n",
    "    for a in list(context.keys()):\n",
    "        for b in list(context.keys()):\n",
    "            if a != b:\n",
    "                context_pairs[\"{0}, {1}\".format(a, b)] = list(set(context[a]).intersection(set(context[b])))\n",
    "    \n",
    "    return context_pairs\n",
    "\n",
    "\n",
    "#STEP 5: define co-occurrence combinations\n",
    "def define_cooccurrence(symbols, context_pairs, gram_freq):\n",
    "    \n",
    "    assert type(context_pairs) == dict\n",
    "    assert type(gram_freq) == dict\n",
    "    assert type(symbols) == list\n",
    "    \n",
    "    co_occur = {}\n",
    "    for k in list(context_pairs.keys()):\n",
    "        for item in context_pairs[k]:\n",
    "            for a in symbols:\n",
    "                for b in symbols:\n",
    "                    x,y = item.split(\", \")[0], item.split(\", \")[1]\n",
    "                    if a == b:\n",
    "                        try:\n",
    "                            n = gram_freq[\"{0}, {1}, {2}\".format(x,a,y)]\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = (n*(n-1))/2\n",
    "                        except:\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = 0.0\n",
    "                        \n",
    "                    elif a != b:\n",
    "                        try:\n",
    "                            n_i = gram_freq[\"{0}, {1}, {2}\".format(x,a,y)]\n",
    "                            n_j = gram_freq[\"{0}, {1}, {2}\".format(x,b,y)]\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = n_i*n_j\n",
    "                        except:\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = 0.0\n",
    "    \n",
    "    return co_occur\n",
    "\n",
    "\n",
    "#STEP 6: Define the count of co-occurrences for symbols a,b for all contexts\n",
    "def co_occur_combos(symbols, con_pairs, co_occurs):\n",
    "    assert type(symbols) == list\n",
    "    assert type(con_pairs) == dict\n",
    "    assert type(co_occurs) == dict\n",
    "    \n",
    "    co_occur_combos = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            total = 0.0\n",
    "            for k in list(con_pairs.keys()):\n",
    "                for item in con_pairs[k]:\n",
    "                    total += co_occurs[\"{0}({1}, {2})\".format(item,a,b)]\n",
    "            co_occur_combos[\"{0}, {1}\".format(a,b)] = total\n",
    "    \n",
    "    return co_occur_combos\n",
    "\n",
    "\n",
    "#STEP 7: Define norm on the count of co-occur combos\n",
    "def define_norm (co_combos):\n",
    "    assert type(co_combos) == dict\n",
    "    norm = 0.0\n",
    "    for k in list(co_combos.keys()):\n",
    "        norm += co_combos[k]\n",
    "    \n",
    "    return norm\n",
    "\n",
    "\n",
    "#STEP 8: Define matrix M over A x A\n",
    "def define_matrix (symbols, co_combos, norm):\n",
    "    assert type(symbols) == list\n",
    "    assert type(co_combos) == dict\n",
    "    assert type(norm) == float\n",
    "    \n",
    "    mat_M = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            mat_M[\"{0}, {1}\".format(a,b)] = co_combos[\"{0}, {1}\".format(a,b)]/norm\n",
    "    \n",
    "    return mat_M\n",
    "\n",
    "\n",
    "#STEP 9: Define the probability of occurrence\n",
    "def prob_occur (symbols, mat_M):\n",
    "    assert type(symbols) == list\n",
    "    assert type(mat_M) == dict\n",
    "    \n",
    "    p = {}\n",
    "    for a in symbols:\n",
    "        total = 0\n",
    "        for b in symbols:\n",
    "            if a != b:\n",
    "                total += mat_M[\"{0}, {1}\".format(a,b)]\n",
    "        total += mat_M[\"{0}, {1}\".format(a,a)]\n",
    "        p[\"{0}\".format(a)] = total\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "#STEP 10: Define the expected values\n",
    "def exp_val (symbols, prob):\n",
    "    assert type(symbols) == list\n",
    "    assert type(prob) == dict\n",
    "    \n",
    "    e_val = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            if a == b:\n",
    "                e_val[\"{0}, {1}\".format(a,b)] = prob[\"{0}\".format(a)]**2\n",
    "            else:\n",
    "                e_val[\"{0}, {1}\".format(a,b)] = 2*prob[\"{0}\".format(a)]*prob[\"{0}\".format(b)]\n",
    "    \n",
    "    return e_val\n",
    "\n",
    "\n",
    "#STEP 11: Define the function for substitution scores\n",
    "def sub_scores (traces):\n",
    "    assert type(traces) == list\n",
    "    \n",
    "    symbols = define_symbols(traces)\n",
    "    three_gs, three_gs_freq = three_grams(traces)\n",
    "    cons = define_context(three_gs)\n",
    "    con_pairs = context_pairs(cons)\n",
    "    co_occurs = define_cooccurrence(symbols, con_pairs, three_gs_freq)\n",
    "    co_combos = co_occur_combos(symbols, con_pairs, co_occurs)\n",
    "    norm = define_norm(co_combos)\n",
    "    matM = define_matrix(symbols, co_combos, norm)\n",
    "    probs = prob_occur(symbols, matM)\n",
    "    e_val = exp_val(symbols, probs)\n",
    "    \n",
    "    sub_costs = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            if a!=b:\n",
    "                try:\n",
    "                    sub_costs[\"{0}, {1}\".format(a,b)] = np.log2(matM[\"{0}, {1}\".format(a,b)]/e_val[\"{0}, {1}\".format(a,b)])\n",
    "                except:\n",
    "                    sub_costs[\"{0}, {1}\".format(a,b)] = -1000\n",
    "    \n",
    "    return sub_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEPS 1-3 are the same for Insertion Score\n",
    "\n",
    "#STEP 4: Define occurence of 3-gram counts\n",
    "def occ_count (symbols, cons, grams, gfreq):\n",
    "    assert type(symbols) == list\n",
    "    assert type(grams) == list\n",
    "    assert type(cons) == dict\n",
    "    \n",
    "    o_counts = {}\n",
    "    for a in list(cons.keys()):\n",
    "        for pair in cons[a]:\n",
    "            x = pair.split(\", \")[0]\n",
    "            y = pair.split(\", \")[1]\n",
    "            o_counts[\"{0}, {1}({2})\".format(x,y,a)] = gfreq[\"{0}, {1}, {2}\".format(x,a,y)]\n",
    "    \n",
    "    return o_counts\n",
    "\n",
    "\n",
    "#STEP 5: define countRgivenL\n",
    "def countRgL (symbols, ocounts):\n",
    "    assert type(symbols) == list\n",
    "    assert type(ocounts) == dict\n",
    "    \n",
    "    rgl_counts = {}\n",
    "    \n",
    "    for a in symbols:\n",
    "        for x in symbols:\n",
    "            #if a !=x:\n",
    "            total = 0\n",
    "            for k in list(ocounts.keys()):\n",
    "                if k.split(\"(\")[0].split(\", \")[0] == x and k.split(\"(\")[1] == \"{0})\".format(a):\n",
    "                    total += ocounts[k]\n",
    "            rgl_counts[\"{0}/{1}\".format(a,x)] = total\n",
    "    \n",
    "    return rgl_counts\n",
    "\n",
    "\n",
    "#STEP 6: define norm(a)\n",
    "def rgl_norm (symbols, rgl_counts):\n",
    "    assert type(symbols) == list\n",
    "    assert type(rgl_counts) == dict\n",
    "    \n",
    "    rgl_norms = {}\n",
    "    \n",
    "    for a in symbols:\n",
    "        total = 0\n",
    "        for x in symbols:\n",
    "            #if a !=x:\n",
    "            total += rgl_counts[\"{0}/{1}\".format(a,x)]\n",
    "        rgl_norms[\"{0}\".format(a)] = total\n",
    "    \n",
    "    return rgl_norms\n",
    "\n",
    "\n",
    "#STEP 7: define the probability of all symbols\n",
    "def rgl_prob (trace):\n",
    "    assert type(trace) == list\n",
    "    \n",
    "    p = {}\n",
    "    for item in trace:\n",
    "        for a in item:\n",
    "            try:\n",
    "                p[\"{0}\".format(a)] += 1\n",
    "            except:\n",
    "                p[\"{0}\".format(a)] = 1\n",
    "    \n",
    "    tot_len = 0\n",
    "    for item in trace:\n",
    "        tot_len += len(item)\n",
    "    \n",
    "    for k in list(p.keys()):\n",
    "        p[k] = p[k]/tot_len\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "#STEP 8: define rglNorm\n",
    "def normed_counts (symbols, rgl, norms):\n",
    "    assert type(symbols) == list\n",
    "    assert type(rgl) == dict\n",
    "    assert type(norms) == dict\n",
    "    \n",
    "    normed_rgls = {}\n",
    "    \n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            normed_rgls[\"{0}/{1}\".format(a,b)] = rgl[\"{0}/{1}\".format(a,b)]/norms[\"{0}\".format(a)]\n",
    "    \n",
    "    return normed_rgls\n",
    "\n",
    "\n",
    "#STEP 9: Define the function for insertion score\n",
    "def insert_scores (traces):\n",
    "    assert type(traces) == list\n",
    "    \n",
    "    symbols = define_symbols(traces)\n",
    "    grams, freq = three_grams(traces)\n",
    "    cons = define_context(grams)\n",
    "    oc = occ_count(symbols, cons, grams, freq)\n",
    "    rgl = countRgL(symbols, oc)\n",
    "    norms = rgl_norm(symbols, rgl)\n",
    "    probs = rgl_prob(traces)\n",
    "    norm_rgls = normed_counts(symbols ,rgl, norms)\n",
    "    \n",
    "    scores = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            scores[\"{0}/{1}\".format(a,b)] = np.log2(norm_rgls[\"{0}/{1}\".format(a,b)]/probs[\"{0}\".format(a)]*probs[\"{0}\".format(b)])\n",
    "    \n",
    "    #replace -inf\n",
    "    for k in list(scores.keys()):\n",
    "        if scores[k] == -np.inf:\n",
    "            scores[k] = -1000\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function definition for calculating similarity\n",
    "def calc_similarity(trace1, trace2, sub_cost, ins_cost, probs):\n",
    "    \n",
    "    assert type(trace1) == type(trace2) == list\n",
    "    \n",
    "    #pad traces\n",
    "    trace1 = [\"_\"] + trace1\n",
    "    trace2 = [\"_\"] + trace2\n",
    "    \n",
    "    #set shorter one as tr1\n",
    "    if len(trace1) > len(trace2):\n",
    "        copy = trace1\n",
    "        trace1 = trace2\n",
    "        trace2 = copy\n",
    "\n",
    "    M = len(trace1)\n",
    "    N = len(trace2)\n",
    "    sim_table = np.zeros((M,N)) #establish table\n",
    "    s_score = sub_cost #get substitution score\n",
    "    ins_score = ins_cost #get insertion score\n",
    "    p = probs #get probabilities\n",
    "    \n",
    "    #fill table, horizontal -> vertical\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            \n",
    "            #original fill horizontal\n",
    "            if i == 0:\n",
    "                if j == 0: #first fill\n",
    "                    sim_table[i][j] = 1000\n",
    "                elif j == 1: #first insert\n",
    "                    sim_table[i][j] = p[\"{0}\".format(trace2[j])]\n",
    "                else: #rest fill, base insert scores\n",
    "                    sim_table[i][j] = ins_score[\"{0}/{1}\".format(trace2[j], trace2[j-1])] + sim_table[i][j-1]\n",
    "            \n",
    "            #original fill vertical\n",
    "            elif j == 0:\n",
    "                if i == 0:#first fill\n",
    "                    sim_table[i][j] = 1000\n",
    "                elif i == 1:\n",
    "                    sim_table[i][j] = p[\"{0}\".format(trace1[i])]\n",
    "                else: #rest fill, base is the opposite of insert scores\n",
    "                    sim_table[i][j] = -1*ins_score[\"{0}/{1}\".format(trace1[i], trace1[i-1])] + sim_table[i-1][j]\n",
    "            \n",
    "            elif trace1[i] == trace2[j]: #no changes\n",
    "                sim_table[i][j] = sim_table[i-1][j-1]\n",
    "            \n",
    "            else: #substitution, insertion or deletion\n",
    "                \n",
    "                #determine the min\n",
    "                op = np.argmax([sim_table[i-1][j], sim_table[i][j-1], sim_table[i-1][j-1]]) #in order, removal, insertion, substitution\n",
    "                if op == 0:\n",
    "                    sim_table[i][j] = -1 + sim_table[i-1][j]#-1*ins_score[\"{0}/{1}\".format(trace2[j],trace1[i])] + sim_table[i-1][j] #removal\n",
    "                elif op == 1:\n",
    "                    sim_table[i][j] = ins_score[\"{0}/{1}\".format(trace2[j],trace1[i])] + sim_table[i][j-1] #insertion\n",
    "                elif op == 2:\n",
    "                    sim_table[i][j] = s_score[\"{0}, {1}\".format(trace1[i],trace2[j])] + sim_table[i-1][j-1] #substitution\n",
    "                \n",
    "    return sim_table[i][j] #final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the function to find all relevant sub-conversations in a trace\n",
    "def find_sub_conversations (trace, labels, c_cap, l):\n",
    "    assert type(trace) == list\n",
    "    assert type(labels) == list\n",
    "    assert type(c_cap) == float\n",
    "    assert type(l) == int\n",
    "    \n",
    "    #counts\n",
    "    sub_convos = {}\n",
    "    \n",
    "    #cycle through label pairs\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            \n",
    "            if i != j: #no repeats\n",
    "                candidate = until_N(trace, [labels[i]], [labels[j]], 50) #cap at 50\n",
    "                \n",
    "                for item in candidate:\n",
    "                    if item[3] > 10 and item [3] < l and item[2] < item[3]*c_cap:\n",
    "                        try:\n",
    "                            sub_convos[\"{0} -> {1}\".format(labels[i], labels[j])] += 1\n",
    "                        except:\n",
    "                            sub_convos[\"{0} -> {1}\".format(labels[i], labels[j])] = 1\n",
    "    \n",
    "    return sub_convos\n",
    "\n",
    "#modify the function to find all relevant sub-conversations in a trace\n",
    "def compare_sub_conversations (trace1, trace2, labels, c_cap, l):\n",
    "    assert type(trace1) == list\n",
    "    assert type(trace2) == list\n",
    "    assert type(labels) == list\n",
    "    assert type(c_cap) == float\n",
    "    assert type(l) == int\n",
    "    \n",
    "    #counts\n",
    "    sub_convos1 = {}\n",
    "    sub_convos2 = {}\n",
    "    \n",
    "    #cycle through label pairs\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            \n",
    "            if i != j: #no repeats\n",
    "                candidate = until_N(trace1, [labels[i]], [labels[j]], 50)\n",
    "                sub_convos1[\"{0} -> {1}\".format(labels[i], labels[j])] = 0\n",
    "                \n",
    "                for item in candidate:\n",
    "                    if item[3] > 10 and item [3] < l and item[2] < item[3]*c_cap:\n",
    "                        sub_convos1[\"{0} -> {1}\".format(labels[i], labels[j])] += 1\n",
    "    \n",
    "    #do for second trace\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "\n",
    "            if i != j: #no repeats\n",
    "                candidate = until_N(trace2, [labels[i]], [labels[j]], 50)\n",
    "                sub_convos2[\"{0} -> {1}\".format(labels[i], labels[j])] = 0\n",
    "\n",
    "                for item in candidate:\n",
    "                    if item[3] > 10 and item [3] < l and item[2] < item[3]*c_cap:\n",
    "                        sub_convos2[\"{0} -> {1}\".format(labels[i], labels[j])] += 1\n",
    "    \n",
    "    #compare and give summary report\n",
    "    report = {}\n",
    "    report_total = 0\n",
    "    for key in list(sub_convos1.keys()):\n",
    "        report[key] = np.abs(sub_convos1[key]-sub_convos2[key])\n",
    "        report_total += report[key]\n",
    "    \n",
    "    return report, report_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The goal is to draw a correlation between similarity of traces (derived from substitution and insertion scores) and the LTL sequences exhibited. To do so, we'll have to address the following considerations:\n",
    "\n",
    "- Similarity measure in the paper originally was done via using event logs from a consistent process (healthcare). Our event logs are not from a consistent process. This could have effects on the similarity (derived from effects on the scores obtained from the eventlogs as \"training\"), so we need to consider the types of processes we pick for our traces.\n",
    "- We need to establish first a way to find LTL sequences that have interesting properties (sequence length and count for violations of N), as those sub-sequences are what we can consider \"sub-conversations\" and are the LTL sequences that are worth looking at in a frequency-based analysis, as well as critical in drawing the correlation we want in this part.\n",
    "\n",
    "# Clustering\n",
    "\n",
    "Presumably, one of the ways we can do this correlation is by looking at the results of clustering, to see if there are correlations between the clusters made by\n",
    "\n",
    "- similarity values alone (Case A)\n",
    "- weighting values alone (Case B)\n",
    "- similarity and weighting values (Case C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using sklearn\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#sample 3 by 3 clustering\n",
    "dis_mat = np.array([[0,1,2],[1,0,3],[2,3,0]])\n",
    "\n",
    "clusters = AgglomerativeClustering(affinity=\"precomputed\", linkage=\"average\").fit(dis_mat)\n",
    "clusters.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kense/.local/lib/python3.6/site-packages/ipykernel_launcher.py:108: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    }
   ],
   "source": [
    "a = [\"a\",\"b\",\"b\",\"a\",\"b\",\"b\",\"a\",\"c\"]\n",
    "b = [\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"c\"]\n",
    "#c = [\"c\",\"c\",\"c\",\"c\",\"c\",\"c\",\"a\",\"c\"]\n",
    "c = [\"a\",\"a\",\"a\",\"b\",\"b\",\"b\",\"c\",\"a\"]\n",
    "scost = sub_scores([a,b,c])\n",
    "icost = insert_scores([a,b,c])\n",
    "prob = rgl_prob([a,b,c])\n",
    "t_run = [a,b,c]\n",
    "dis_mat = np.zeros((len(t_run),len(t_run)))\n",
    "\n",
    "for i in range(len(t_run)):\n",
    "    for j in range(len(t_run)):\n",
    "        dis_mat[i][j] = 1/(1000+calc_similarity(t_run[i],t_run[j], scost, icost, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = AgglomerativeClustering(affinity=\"precomputed\", linkage=\"average\").fit(dis_mat)\n",
    "clusters.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0005    , 0.0005007 , 0.00050052],\n",
       "       [0.0005007 , 0.0005    , 0.00100173],\n",
       "       [0.00050061, 0.00050072, 0.0005    ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data\n",
    "dat1 = pd.read_csv('./graham.norton.s22.e08_data.csv')\n",
    "dat2 = pd.read_csv('./graham.norton.s22.e12_data.csv')\n",
    "dat3 = pd.read_csv('./blackpink_data.csv')\n",
    "test1 = list(dat1.L)\n",
    "test2 = list(dat2.L)\n",
    "test3 = list(dat3.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kense/.local/lib/python3.6/site-packages/ipykernel_launcher.py:108: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0005     0.0007274  0.00131518]\n",
      " [0.0007274  0.0005     0.00191831]\n",
      " [0.00131518 0.00191831 0.0005    ]]\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "eventlog = [test1,test2,test3]\n",
    "scost = sub_scores(eventlog)\n",
    "icost = insert_scores(eventlog)\n",
    "prob = rgl_prob(eventlog)\n",
    "dis_mat = np.zeros((len(eventlog), len(eventlog)))\n",
    "\n",
    "for i in range(len(eventlog)):\n",
    "    for j in range(len(eventlog)):\n",
    "        dis_mat[i][j] = 1/(1000+calc_similarity(eventlog[i],eventlog[j], scost, icost, prob))\n",
    "\n",
    "print(dis_mat)\n",
    "clusters = AgglomerativeClustering(affinity=\"precomputed\", linkage=\"average\").fit(dis_mat)\n",
    "print(clusters.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighting values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
