{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit-distance operations are defined as \n",
    "* (a,a) denotes a match of symbols at the given position\n",
    "* (a,-) denotes deletion of symbol 'a' at some position\n",
    "* (-,b) denotes insertion of symbol 'b' at some position\n",
    "* (a,b) denotes replacement of 'a' with 'b' at some position, and a != b\n",
    "We assign a separate cost for each of these operations according to our needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for simplicity and completeness sake, we define a simple levenshtein distance function first\n",
    "\n",
    "def basic_distance (trace1, trace2):\n",
    "\n",
    "    M = len(trace1)\n",
    "    N = len(trace2)\n",
    "    edit_table = np.zeros((M,N)) #establish table\n",
    "\n",
    "    #fill table\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "\n",
    "            if i == 0:\n",
    "                edit_table[i][j] = j\n",
    "\n",
    "            elif j ==0:\n",
    "                edit_table[i][j] = i\n",
    "\n",
    "            elif trace1[i-1] == trace2[j-1]:\n",
    "                edit_table[i][j] = edit_table[i-1][j-1]\n",
    "\n",
    "            else:\n",
    "                edit_table[i][j] = 1 + min(edit_table[i-1][j], edit_table[i][j-1], edit_table[i-1][j-1]) #scoring done here\n",
    "\n",
    "    return edit_table[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./data/graham.norton.s22.e08_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cedd6459fcf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#sample test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdat1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/graham.norton.s22.e08_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdat2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/graham.norton.s22.e12_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdat3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/blackpink_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./data/graham.norton.s22.e08_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "#sample test\n",
    "dat1 = pd.read_csv('./data/graham.norton.s22.e08_data.csv')\n",
    "dat2 = pd.read_csv('./data/graham.norton.s22.e12_data.csv')\n",
    "dat3 = pd.read_csv('./data/blackpink_data.csv')\n",
    "test1 = list(dat1.L)\n",
    "test2 = list(dat2.L)\n",
    "test3 = list(dat3.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_distance(test1,test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "c = [\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "b = [\"b\",\"b\",\"b\",\"b\",\"b\"]\n",
    "d = [\"a\",\"a\",\"a\",\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "basic_distance(a,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"c\",\"c\"]\n",
    "b = [\"b\",\"b\",\"b\",\"b\",\"a\",\"b\",\"c\",\"c\",\"c\",\"c\"]\n",
    "basic_distance(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_distance(test1, test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline\n",
    "import nltk\n",
    "nltk.edit_distance(test1,test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = [\"a\",\"b\"]\n",
    "t2 = [\"a\",\"b\",\"c\",\"d\"]\n",
    "nltk.edit_distance(t1,t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have to alter the function to accomodate for different scoring metrics according to the paper\n",
    "* substitution of uncorrelated activities should be discouraged\n",
    "* substitution of contrasting activities should be penalized\n",
    "* insertion of activities out of context should be discouraged\n",
    "* substitution of correlated activities should be encouraged in proportion to the degree of similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: Define the symbols in the list of traces\n",
    "def define_symbols (traces):\n",
    "    assert type(traces) == list\n",
    "    symbols = []\n",
    "    for item in traces:\n",
    "        symbols.append(set(item))\n",
    "    x = symbols[0]\n",
    "    for i in range(len(symbols)):\n",
    "        x = x.union(symbols[i])\n",
    "    \n",
    "    return list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A != B\n",
    "a = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "b = [\"a\",\"g\",\"q\",\"q\",\"e\",\"f\"]\n",
    "c = [a, b]\n",
    "print(define_symbols(c))\n",
    "\n",
    "# A == B\n",
    "a = [\"a\", \"b\", \"c\", \"d\"]\n",
    "b = [\"a\", \"a\", \"c\", \"d\", \"b\", \"b\"]\n",
    "c = [a,b]\n",
    "print(define_symbols(c))\n",
    "\n",
    "# on the dataset\n",
    "c = [test1,test2]\n",
    "print(define_symbols(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: Define the set of all 3-grams in the logs and their frequencies\n",
    "def three_grams (traces):\n",
    "    assert type(traces) == list\n",
    "    g3 = []\n",
    "    g3_freq = {}\n",
    "    for trace in traces:\n",
    "        for i in range(len(trace)-2):\n",
    "            g3.append(\", \".join(list(trace[i:i+3])))\n",
    "            try:\n",
    "                g3_freq[\", \".join(list(trace[i:i+3]))] += 1\n",
    "            except:\n",
    "                g3_freq[\", \".join(list(trace[i:i+3]))] = 1\n",
    "    return list(set(g3)), g3_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A = abcdefghijk, B = kjihgfedcba\n",
    "a = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\"]\n",
    "b = [\"k\",\"j\",\"i\",\"h\",\"g\",\"f\",\"e\",\"d\",\"c\",\"b\",\"a\"]\n",
    "c = [a,b]\n",
    "print(three_grams(c))\n",
    "\n",
    "#A = abcdefghijk, B = abcdefghijk\n",
    "a = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\"]\n",
    "b = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\"]\n",
    "c = [a,b]\n",
    "print(three_grams(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3: Define the context for symbol a\n",
    "def define_context(grams):\n",
    "    \n",
    "    assert type(grams) == list\n",
    "    \n",
    "    context = {}\n",
    "    for gram in grams:\n",
    "        x,a,y = gram.split(\", \")\n",
    "        try:\n",
    "            context[a].append(\"{0}, {1}\".format(x,y))\n",
    "        except:\n",
    "            context[a] = []\n",
    "            context[a].append(\"{0}, {1}\".format(x,y))\n",
    "            \n",
    "    #clear dups\n",
    "    for k in list(context.keys()):\n",
    "        context[k] = list(set(context[k]))\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test context, should have keys a and b, where each has aa and bb respectively\n",
    "a = [\"a\",\"a\",\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "b = [\"b\",\"b\",\"b\",\"b\",\"b\"]\n",
    "c = [a,b]\n",
    "grams, g3_freq = three_grams(c)\n",
    "print(define_context(grams))\n",
    "\n",
    "#test context\n",
    "a = [\"a\",\"a\",\"a\",\"a\",\"a\",\"a\",\"a\",\"c\",\"e\",\"f\",\"a\",\"a\",\"b\",\"c\"]\n",
    "b = [\"b\",\"b\",\"b\",\"b\",\"b\",\"c\",\"c\",\"a\"]\n",
    "c = [a,b]\n",
    "grams, g3_freq = three_grams(c)\n",
    "print(define_context(grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4: define pairs of context\n",
    "def context_pairs (context):\n",
    "    \n",
    "    assert type(context) == dict\n",
    "    \n",
    "    context_pairs = {}\n",
    "    for a in list(context.keys()):\n",
    "        for b in list(context.keys()):\n",
    "            if a != b:\n",
    "                context_pairs[\"{0}, {1}\".format(a, b)] = list(set(context[a]).intersection(set(context[b])))\n",
    "    \n",
    "    return context_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test context pairs, should be empty\n",
    "a = [\"a\",\"a\",\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "b = [\"b\",\"b\",\"b\",\"b\",\"b\"]\n",
    "c = [a,b]\n",
    "grams, g3_freq = three_grams(c)\n",
    "context = define_context(grams)\n",
    "print(context_pairs(context))\n",
    "\n",
    "#test context pairs, should be aa and bb\n",
    "a = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\"]\n",
    "b = [\"b\",\"b\",\"b\",\"b\",\"a\",\"b\"]\n",
    "c = [a,b]\n",
    "grams, g3_freq = three_grams(c)\n",
    "context = define_context(grams)\n",
    "print(context_pairs(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 5: define co-occurrence combinations\n",
    "def define_cooccurrence(symbols, context_pairs, gram_freq):\n",
    "    \n",
    "    assert type(context_pairs) == dict\n",
    "    assert type(gram_freq) == dict\n",
    "    assert type(symbols) == list\n",
    "    \n",
    "    co_occur = {}\n",
    "    for k in list(context_pairs.keys()):\n",
    "        for item in context_pairs[k]:\n",
    "            for a in symbols:\n",
    "                for b in symbols:\n",
    "                    x,y = item.split(\", \")[0], item.split(\", \")[1]\n",
    "                    if a == b:\n",
    "                        try:\n",
    "                            n = gram_freq[\"{0}, {1}, {2}\".format(x,a,y)]\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = (n*(n-1))/2\n",
    "                        except:\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = 0.0\n",
    "                        \n",
    "                    elif a != b:\n",
    "                        try:\n",
    "                            n_i = gram_freq[\"{0}, {1}, {2}\".format(x,a,y)]\n",
    "                            n_j = gram_freq[\"{0}, {1}, {2}\".format(x,b,y)]\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = n_i*n_j\n",
    "                        except:\n",
    "                            co_occur[\"{0}, {1}({2}, {3})\".format(x,y,a,b)] = 0.0\n",
    "    \n",
    "    return co_occur\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "a = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"c\"]\n",
    "b = [\"b\",\"b\",\"b\",\"b\",\"a\",\"b\",\"c\"]\n",
    "c = [a,b]\n",
    "grams, g3_freq = three_grams(c)\n",
    "print(grams)\n",
    "print(g3_freq)\n",
    "context = define_context(grams)\n",
    "print(context)\n",
    "con_pairs = context_pairs(context)\n",
    "print(con_pairs)\n",
    "print(define_cooccurrence(define_symbols(c),con_pairs,g3_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 6: Define the count of co-occurrences for symbols a,b for all contexts\n",
    "def co_occur_combos(symbols, con_pairs, co_occurs):\n",
    "    assert type(symbols) == list\n",
    "    assert type(con_pairs) == dict\n",
    "    assert type(co_occurs) == dict\n",
    "    \n",
    "    co_occur_combos = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            total = 0.0\n",
    "            for k in list(con_pairs.keys()):\n",
    "                for item in con_pairs[k]:\n",
    "                    total += co_occurs[\"{0}({1}, {2})\".format(item,a,b)]\n",
    "            co_occur_combos[\"{0}, {1}\".format(a,b)] = total\n",
    "    \n",
    "    return co_occur_combos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "a = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"b\"]\n",
    "b = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\"]\n",
    "c = [a,b]\n",
    "grams, g3_freq = three_grams(c)\n",
    "print(g3_freq)\n",
    "context = define_context(grams)\n",
    "print(context)\n",
    "con_pairs = context_pairs(context)\n",
    "print(con_pairs)\n",
    "co_occurs = define_cooccurrence(define_symbols(c),con_pairs,g3_freq)\n",
    "print(co_occurs)\n",
    "print(co_occur_combos(define_symbols(c),con_pairs,co_occurs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 7: Define norm on the count of co-occur combos\n",
    "def define_norm (co_combos):\n",
    "    assert type(co_combos) == dict\n",
    "    norm = 0.0\n",
    "    for k in list(co_combos.keys()):\n",
    "        norm += co_combos[k]\n",
    "    \n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "a = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"b\"]\n",
    "b = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\"]\n",
    "c = [a,b]\n",
    "grams, g3_freq = three_grams(c)\n",
    "context = define_context(grams)\n",
    "con_pairs = context_pairs(context)\n",
    "co_occurs = define_cooccurrence(define_symbols(c),con_pairs,g3_freq)\n",
    "print(define_norm(co_occur_combos(define_symbols(c),con_pairs,co_occurs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 8: Define matrix M over A x A\n",
    "def define_matrix (symbols, co_combos, norm):\n",
    "    assert type(symbols) == list\n",
    "    assert type(co_combos) == dict\n",
    "    assert type(norm) == float\n",
    "    \n",
    "    mat_M = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            mat_M[\"{0}, {1}\".format(a,b)] = co_combos[\"{0}, {1}\".format(a,b)]/norm\n",
    "    \n",
    "    return mat_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"b\"]\n",
    "b = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\"]\n",
    "c = [a,b]\n",
    "grams, g3_freq = three_grams(c)\n",
    "context = define_context(grams)\n",
    "con_pairs = context_pairs(context)\n",
    "co_occurs = define_cooccurrence(define_symbols(c),con_pairs,g3_freq)\n",
    "norm = define_norm(co_occur_combos(define_symbols(c),con_pairs,co_occurs))\n",
    "co_combos = co_occur_combos(define_symbols(c), con_pairs, co_occurs)\n",
    "print(define_matrix(define_symbols(c), co_combos, norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_occur (symbols, mat_M):\n",
    "    assert type(symbols) == list\n",
    "    assert type(mat_M) == dict\n",
    "    \n",
    "    p = {}\n",
    "    for a in symbols:\n",
    "        total = 0\n",
    "        for b in symbols:\n",
    "            if a != b:\n",
    "                total += mat_M[\"{0}, {1}\".format(a,b)]\n",
    "        total += mat_M[\"{0}, {1}\".format(a,a)]\n",
    "        p[\"{0}\".format(a)] = total\n",
    "    \n",
    "    return p\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"b\"]\n",
    "b = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\"]\n",
    "c = [a,b]\n",
    "grams, g3_freq = three_grams(c)\n",
    "context = define_context(grams)\n",
    "con_pairs = context_pairs(context)\n",
    "co_occurs = define_cooccurrence(define_symbols(c),con_pairs,g3_freq)\n",
    "norm = define_norm(co_occur_combos(define_symbols(c),con_pairs,co_occurs))\n",
    "co_combos = co_occur_combos(define_symbols(c), con_pairs, co_occurs)\n",
    "matM = define_matrix(define_symbols(c), co_combos, norm)\n",
    "print(prob_occur(define_symbols(c), matM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_val (symbols, prob):\n",
    "    assert type(symbols) == list\n",
    "    assert type(prob) == dict\n",
    "    \n",
    "    e_val = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            if a == b:\n",
    "                e_val[\"{0}, {1}\".format(a,b)] = prob[\"{0}\".format(a)]**2\n",
    "            else:\n",
    "                e_val[\"{0}, {1}\".format(a,b)] = 2*prob[\"{0}\".format(a)]*prob[\"{0}\".format(b)]\n",
    "    \n",
    "    return e_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"b\"]\n",
    "b = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\"]\n",
    "c = [a,b]\n",
    "grams, g3_freq = three_grams(c)\n",
    "context = define_context(grams)\n",
    "con_pairs = context_pairs(context)\n",
    "co_occurs = define_cooccurrence(define_symbols(c),con_pairs,g3_freq)\n",
    "norm = define_norm(co_occur_combos(define_symbols(c),con_pairs,co_occurs))\n",
    "co_combos = co_occur_combos(define_symbols(c), con_pairs, co_occurs)\n",
    "matM = define_matrix(define_symbols(c), co_combos, norm)\n",
    "probs = prob_occur(define_symbols(c), matM)\n",
    "print(exp_val(define_symbols(c), probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_scores (traces):\n",
    "    assert type(traces) == list\n",
    "    \n",
    "    symbols = define_symbols(traces)\n",
    "    three_gs, three_gs_freq = three_grams(traces)\n",
    "    cons = define_context(three_gs)\n",
    "    con_pairs = context_pairs(cons)\n",
    "    co_occurs = define_cooccurrence(symbols, con_pairs, three_gs_freq)\n",
    "    co_combos = co_occur_combos(symbols, con_pairs, co_occurs)\n",
    "    norm = define_norm(co_combos)\n",
    "    matM = define_matrix(symbols, co_combos, norm)\n",
    "    probs = prob_occur(symbols, matM)\n",
    "    e_val = exp_val(symbols, probs)\n",
    "    \n",
    "    sub_costs = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            if a!=b:\n",
    "                try:\n",
    "                    sub_costs[\"{0}, {1}\".format(a,b)] = np.log2(matM[\"{0}, {1}\".format(a,b)]/e_val[\"{0}, {1}\".format(a,b)])\n",
    "                except:\n",
    "                    sub_costs[\"{0}, {1}\".format(a,b)] = -1000\n",
    "    \n",
    "    return sub_costs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s_score = sub_scores([test1, test2,test3])\n",
    "s_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gut check\n",
    "print(sub_scores([test1, test2]) == sub_scores([test2, test1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note that with very broad categories of labels, there is going to be little defined similarity between each label, and switching the labels will always be bad (thus negative score for all substitutions), however this is expected, and we can see something like, what are the least harmful switches, to show similarity. Notice closed.question and open.question have higher similarity, which is something expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insertion STEP 4: define Cxy(a) as the count of occurrences of 3-gram xay\n",
    "def occ_count (symbols, cons, grams, gfreq):\n",
    "    assert type(symbols) == list\n",
    "    assert type(grams) == list\n",
    "    assert type(cons) == dict\n",
    "    \n",
    "    o_counts = {}\n",
    "    for a in list(cons.keys()):\n",
    "        for pair in cons[a]:\n",
    "            x = pair.split(\", \")[0]\n",
    "            y = pair.split(\", \")[1]\n",
    "            o_counts[\"{0}, {1}({2})\".format(x,y,a)] = gfreq[\"{0}, {1}, {2}\".format(x,a,y)]\n",
    "    \n",
    "    return o_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test occ_count with values easily verifiable\n",
    "t1 = [\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "t2 = [\"a\",\"a\",\"a\",\"a\",\"b\"]\n",
    "tlog = [t1, t2]\n",
    "symbols = define_symbols(tlog)\n",
    "grams, gfreq = three_grams(tlog)\n",
    "cons = define_context(grams)\n",
    "oc = occ_count(symbols, cons, grams, gfreq)\n",
    "print(oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insertion STEP 5: define countRgivenL\n",
    "def countRgL (symbols, ocounts):\n",
    "    assert type(symbols) == list\n",
    "    assert type(ocounts) == dict\n",
    "    \n",
    "    rgl_counts = {}\n",
    "    \n",
    "    for a in symbols:\n",
    "        for x in symbols:\n",
    "            #if a !=x:\n",
    "            total = 0\n",
    "            for k in list(ocounts.keys()):\n",
    "                if k.split(\"(\")[0].split(\", \")[0] == x and k.split(\"(\")[1] == \"{0})\".format(a):\n",
    "                    total += ocounts[k]\n",
    "            rgl_counts[\"{0}/{1}\".format(a,x)] = total\n",
    "    \n",
    "    return rgl_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with values easily verifiable\n",
    "t1 = [\"a\",\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "t2 = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\"]\n",
    "tlog = [t1, t2]\n",
    "symbols = define_symbols(tlog)\n",
    "grams, gfreq = three_grams(tlog)\n",
    "cons = define_context(grams)\n",
    "oc = occ_count(symbols, cons, grams, gfreq)\n",
    "rgl = countRgL(symbols, oc)\n",
    "print(rgl)\n",
    "\n",
    "#should add c to the results table, but no occurrences since it's at the end, opens 1 more for a/b\n",
    "t1 = [\"a\",\"a\",\"a\",\"a\",\"a\",\"a\",\"c\"]\n",
    "t2 = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\",\"c\"]\n",
    "tlog = [t1, t2]\n",
    "symbols = define_symbols(tlog)\n",
    "grams, gfreq = three_grams(tlog)\n",
    "cons = define_context(grams)\n",
    "oc = occ_count(symbols, cons, grams, gfreq)\n",
    "rgl = countRgL(symbols, oc)\n",
    "print(rgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insertion STEP 6: define norm(a)\n",
    "def rgl_norm (symbols, rgl_counts):\n",
    "    assert type(symbols) == list\n",
    "    assert type(rgl_counts) == dict\n",
    "    \n",
    "    rgl_norms = {}\n",
    "    \n",
    "    for a in symbols:\n",
    "        total = 0\n",
    "        for x in symbols:\n",
    "            #if a !=x:\n",
    "            total += rgl_counts[\"{0}/{1}\".format(a,x)]\n",
    "        rgl_norms[\"{0}\".format(a)] = total\n",
    "    \n",
    "    return rgl_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with values easily verifiable\n",
    "t1 = [\"a\",\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "t2 = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\"]\n",
    "tlog = [t1, t2]\n",
    "symbols = define_symbols(tlog)\n",
    "grams, gfreq = three_grams(tlog)\n",
    "cons = define_context(grams)\n",
    "oc = occ_count(symbols, cons, grams, gfreq)\n",
    "rgl = countRgL(symbols, oc)\n",
    "norms = rgl_norm(symbols, rgl)\n",
    "print(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insertion STEP 7: define the probability of all symbols\n",
    "def rgl_prob (trace):\n",
    "    assert type(trace) == list\n",
    "    \n",
    "    p = {}\n",
    "    for item in trace:\n",
    "        for a in item:\n",
    "            try:\n",
    "                p[\"{0}\".format(a)] += 1\n",
    "            except:\n",
    "                p[\"{0}\".format(a)] = 1\n",
    "    \n",
    "    tot_len = 0\n",
    "    for item in trace:\n",
    "        tot_len += len(item)\n",
    "    \n",
    "    for k in list(p.keys()):\n",
    "        p[k] = p[k]/tot_len\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with values easily verifiable\n",
    "t1 = [\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "t2 = [\"a\",\"a\",\"a\",\"a\",\"b\"]\n",
    "tlog = [t1, t2]\n",
    "symbols = define_symbols(tlog)\n",
    "grams, gfreq = three_grams(tlog)\n",
    "cons = define_context(grams)\n",
    "oc = occ_count(symbols, cons, grams, gfreq)\n",
    "rgl = countRgL(symbols, oc)\n",
    "norms = rgl_norm(symbols, rgl)\n",
    "probs = rgl_prob(tlog)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insertion STEP 8: define rglNorm\n",
    "def normed_counts (symbols, rgl, norms):\n",
    "    assert type(symbols) == list\n",
    "    assert type(rgl) == dict\n",
    "    assert type(norms) == dict\n",
    "    \n",
    "    normed_rgls = {}\n",
    "    \n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            normed_rgls[\"{0}/{1}\".format(a,b)] = rgl[\"{0}/{1}\".format(a,b)]/norms[\"{0}\".format(a)]\n",
    "    \n",
    "    return normed_rgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with values easily verifiable\n",
    "t1 = [\"a\",\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "t2 = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\"]\n",
    "tlog = [t1, t2]\n",
    "symbols = define_symbols(tlog)\n",
    "grams, gfreq = three_grams(tlog)\n",
    "cons = define_context(grams)\n",
    "oc = occ_count(symbols, cons, grams, gfreq)\n",
    "rgl = countRgL(symbols, oc)\n",
    "norms = rgl_norm(symbols, rgl)\n",
    "probs = rgl_prob(tlog)\n",
    "norm_rgls = normed_counts(symbols, rgl, norms)\n",
    "print(norm_rgls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_scores (traces):\n",
    "    assert type(traces) == list\n",
    "    \n",
    "    symbols = define_symbols(traces)\n",
    "    grams, freq = three_grams(traces)\n",
    "    cons = define_context(grams)\n",
    "    oc = occ_count(symbols, cons, grams, freq)\n",
    "    rgl = countRgL(symbols, oc)\n",
    "    norms = rgl_norm(symbols, rgl)\n",
    "    probs = rgl_prob(traces)\n",
    "    norm_rgls = normed_counts(symbols ,rgl, norms)\n",
    "    \n",
    "    scores = {}\n",
    "    for a in symbols:\n",
    "        for b in symbols:\n",
    "            scores[\"{0}/{1}\".format(a,b)] = np.log2(norm_rgls[\"{0}/{1}\".format(a,b)]/probs[\"{0}\".format(a)]*probs[\"{0}\".format(b)])\n",
    "    \n",
    "    #replace -inf\n",
    "    for k in list(scores.keys()):\n",
    "        if scores[k] == -np.inf:\n",
    "            scores[k] = -1000\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = [\"a\",\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "t2 = [\"a\",\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "tlog = [t1, t2]\n",
    "print(insert_scores(tlog))\n",
    "\n",
    "t1 = [\"a\",\"a\",\"a\",\"a\",\"a\",\"a\"]\n",
    "t2 = [\"a\",\"a\",\"a\",\"a\",\"b\",\"a\"]\n",
    "tlog = [t1, t2]\n",
    "print(insert_scores(tlog))\n",
    "\n",
    "t1 = [\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"c\",\"c\"]\n",
    "t2 = [\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\"]\n",
    "tlog = [t1, t2]\n",
    "print(insert_scores(tlog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here this seems correct since adding b given a will make t1 more similar to t2, where the only difference is that t2 has an extra b!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test on our logs, specifically looking for respond.agree, open.question or respond.agree, closed.question to have high scores, since they usually\n",
    "#appear next to each other in that order\n",
    "ins_score = insert_scores([test1,test2,test3])\n",
    "ins_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scores generally make sense\n",
    "\n",
    "# we can define similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gutcheck\n",
    "insert_scores([test1,test2]) == insert_scores([test2, test1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cost(cost):\n",
    "    #convert cost to a distance penalty\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_similarity(trace1, trace2, sub_cost, ins_cost, probs):\n",
    "    \n",
    "    assert type(trace1) == type(trace2) == list\n",
    "    \n",
    "    #pad traces\n",
    "    trace1 = [\"_\"] + trace1\n",
    "    trace2 = [\"_\"] + trace2\n",
    "    \n",
    "    #set shorter one as tr1\n",
    "    if len(trace1) > len(trace2):\n",
    "        copy = trace1\n",
    "        trace1 = trace2\n",
    "        trace2 = copy\n",
    "\n",
    "    M = len(trace1)\n",
    "    N = len(trace2)\n",
    "    sim_table = np.zeros((M,N)) #establish table\n",
    "    s_score = sub_cost #get substitution score\n",
    "    ins_score = ins_cost #get insertion score\n",
    "    p = probs #get probabilities\n",
    "    \n",
    "    #fill table, horizontal -> vertical\n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            \n",
    "            #original fill horizontal\n",
    "            if i == 0:\n",
    "                if j == 0: #first fill\n",
    "                    sim_table[i][j] = 1000\n",
    "                elif j == 1: #first insert\n",
    "                    sim_table[i][j] = p[\"{0}\".format(trace2[j])]\n",
    "                else: #rest fill, base insert scores\n",
    "                    sim_table[i][j] = ins_score[\"{0}/{1}\".format(trace2[j], trace2[j-1])] + sim_table[i][j-1]\n",
    "            \n",
    "            #original fill vertical\n",
    "            elif j == 0:\n",
    "                if i == 0:#first fill\n",
    "                    sim_table[i][j] = 1000\n",
    "                elif i == 1:\n",
    "                    sim_table[i][j] = p[\"{0}\".format(trace1[i])]\n",
    "                else: #rest fill, base is the opposite of insert scores\n",
    "                    sim_table[i][j] = -1*ins_score[\"{0}/{1}\".format(trace1[i], trace1[i-1])] + sim_table[i-1][j]\n",
    "            \n",
    "            elif trace1[i] == trace2[j]: #no changes\n",
    "                sim_table[i][j] = sim_table[i-1][j-1]\n",
    "            \n",
    "            else: #substitution, insertion or deletion\n",
    "                \n",
    "                #determine the min\n",
    "                op = np.argmax([sim_table[i-1][j], sim_table[i][j-1], sim_table[i-1][j-1]]) #in order, removal, insertion, substitution\n",
    "                if op == 0:\n",
    "                    sim_table[i][j] = -1 + sim_table[i-1][j]#-1*ins_score[\"{0}/{1}\".format(trace2[j],trace1[i])] + sim_table[i-1][j] #removal\n",
    "                elif op == 1:\n",
    "                    sim_table[i][j] = ins_score[\"{0}/{1}\".format(trace2[j],trace1[i])] + sim_table[i][j-1] #insertion\n",
    "                elif op == 2:\n",
    "                    sim_table[i][j] = s_score[\"{0}, {1}\".format(trace1[i],trace2[j])] + sim_table[i-1][j-1] #substitution\n",
    "                \n",
    "    return sim_table[i][j] #final score\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = [\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"c\",\"c\"]\n",
    "t2 = [\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"a\",\"a\"]\n",
    "t3 = [\"a\",\"a\",\"a\",\"a\",\"b\",\"c\",\"c\",\"c\",\"c\",\"c\"]\n",
    "t4 = [\"a\",\"a\",\"a\"]\n",
    "t5 = [\"b\",\"b\",\"a\",\"b\"]\n",
    "t6 = [\"c\",\"b\",\"b\",\"b\",\"b\",\"c\",\"b\",\"c\",\"c\",\"b\",\"a\",\"b\",\"c\",\"c\",\"a\",\"a\",\"b\",\"c\"]\n",
    "t7 = [\"c\",\"c\",\"c\",\"c\",\"c\",\"c\",\"c\",\"a\",\"a\",\"b\"]\n",
    "t8 = [\"c\",\"c\",\"c\",\"c\"]\n",
    "t9 = [\"b\",\"a\",\"a\",\"b\",\"a\",\"a\",\"b\",\"c\"]\n",
    "t10 = [\"a\",\"c\",\"c\",\"b\",\"a\",\"c\",\"c\",\"b\"]\n",
    "tset = [t1,t2,t3,t4,t5,t6,t7,t8,t9,t10]\n",
    "ssc = sub_scores(tset)\n",
    "insc = insert_scores(tset)\n",
    "prob = rgl_prob(tset)\n",
    "\n",
    "a = [\"a\",\"c\",\"c\",\"b\",\"b\",\"a\",\"b\",\"b\",\"c\",\"c\"]\n",
    "b = [\"b\",\"c\",\"c\",\"a\",\"a\",\"a\",\"c\",\"c\",\"a\",\"b\",\"b\"]\n",
    "c = [\"a\",\"c\",\"c\",\"b\",\"b\",\"a\",\"b\",\"b\",\"c\",\"c\"]\n",
    "sim1 = calc_similarity(a,b,ssc,insc,prob)\n",
    "sim2 = calc_similarity(a,c,ssc,insc,prob)\n",
    "print(sim1)\n",
    "print(sim2) #we'll have to account for the fact that 0 is a special case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tset1 = [test1, test2, test3]\n",
    "ssc1 = sub_scores(tset1)\n",
    "insc1 = insert_scores(tset1)\n",
    "prob1 = rgl_prob(tset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim3 = calc_similarity(test1, test2, ssc1, insc1, prob1)\n",
    "sim4 = calc_similarity(test1, test3, ssc1, insc1, prob1)\n",
    "print(sim3)\n",
    "print(sim4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# indeed, trace1 should be closer to trace2 than it is to trace3!\n",
    "\n",
    "# Next, we can consider things like defined sub-conversations (trends of labels that are predetermined) and their frequencies. The intuition being that traces with similar label occurrences only address short term connections rather than long term trends, adding freq for long term trends should also assist in telling us if the \"genre\" or the \"flow\" of two traces are similar, which is also a valuable metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
